{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d407953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_results_validated = pd.read_csv('test_folds_validated.csv', sep = '\\t')\n",
    "bin_results = pd.read_csv('acsess_class_results_folds_test.csv', sep = '\\t')\n",
    "total_adjucated = pd.read_csv('total_adjucated_with_preann_PS.csv', sep = '\\t')\n",
    "#fold_5 = pd.read_csv('acsess_bin_results_folds_test_fold5only.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f957cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_adjucated.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b7401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_adjucated[['relevance_manual', 'note_PS_manual']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68d49d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_adjucated[total_adjucated['note_nr'].isin([6346,  42269,  53631,  70592,  82775,  93555, 130336, 133017, 137673, 150060])][['note_nr', 'note_PS_manual']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cff7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_adjucated[total_adjucated['note_nr'].isin([6346,  42269,  53631,  70592,  82775,  93555, 130336, 133017, 137673, 150060])][['note_nr', 'note_PS_manual', 'previous_ann_PS']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1797d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_adjucated['note_nr'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a383e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_adjucated['previous_ann_PS'] = total_adjucated['previous_ann']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cf27e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_adjucated['previous_ann'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf4959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import clear_output\n",
    "\n",
    "# extracted_PS = []\n",
    "\n",
    "# for index, row in total_adjucated.iterrows():\n",
    "#     if row['previous_ann'] == 1:\n",
    "#         while True:\n",
    "#             try:\n",
    "#                 print(\"Index:\", index)\n",
    "#                 print('Sentence:', row['pseudonomised_text'], 'Label:', row['previous_ann'])\n",
    "#                 user_input = float(input(\"Please enter an integer: \"))\n",
    "#                 clear_output(wait=True)\n",
    "#                 break\n",
    "#             except:\n",
    "#                 print(\"TRY AGAIN:\")\n",
    "#     elif row['previous_ann'] != 1:\n",
    "#         user_input = 0\n",
    "#     extracted_PS.append(user_input)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b489a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_adjucated['previous_ann_PS'] = extracted_PS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b0b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_adjucated.to_csv('total_adjucated_with_preann_PS.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ba0d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bin_results['validated'] = test_results_validated['validated']\n",
    "#bin_results['binary k=5'] = fold_5['binary k=5']\n",
    "bin_results['binary k=2'] = [0] * len(bin_results)\n",
    "#bin_results['binary k=3'] = [0] * len(bin_results)\n",
    "#bin_results['binary k=4'] = [0] * len(bin_results)\n",
    "\n",
    "bin_results.columns = bin_results.columns.str.replace('binary ', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c0c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin_results[['k=0', 'k=1', 'k=5']] = bin_results[['k=0', 'k=1', 'k=5']].applymap(lambda x: 1 if x > 1 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81287a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin_results.dropna()\n",
    "#print(bin_results.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709dc64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_results = bin_results[bin_results['relevance_manual'] == bin_results['validated']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c49e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(bin_results)\n",
    "#print(bin_results['note_nr'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3450ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_adjucated = total_adjucated[total_adjucated['note_nr'].isin(bin_results['note_nr'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e426a100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(total_adjusted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9308e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_results['Baseline all 0'] = len(bin_results) * [0]\n",
    "bin_results = bin_results[bin_results['relevance_manual'] == 1]\n",
    "total_adjucated = total_adjucated[total_adjucated['relevance_manual'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c8e4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_adjucated[['relevance_manual', 'note_PS_manual']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5c6bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped = bin_results.groupby('note_nr')\n",
    "#grouped_bin = grouped.sum()\n",
    "\n",
    "grouped_max = bin_results.groupby('note_nr').agg({'k=0': 'max', 'k=1': 'max', 'k=2': 'max', 'k=3': 'max', 'k=4': 'max', 'k=5': 'max'}).reset_index()\n",
    "grouped_mean = bin_results.groupby('note_nr').agg({'k=0': 'mean', 'k=1': 'mean', 'k=2': 'mean', 'k=3': 'mean', 'k=4': 'mean','k=5': 'mean'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee51c48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_max = grouped_max[['note_nr', 'k=0', 'k=1', 'k=2', 'k=3', 'k=4', 'k=5']]\n",
    "grouped_mean = grouped_mean[['note_nr', 'k=0', 'k=1', 'k=2', 'k=3', 'k=4', 'k=5']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43838144",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grouped_max['note_nr'].unique())\n",
    "print(grouped_mean['note_nr'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f4307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(grouped_max['manual_sentence_labels'])\n",
    "#print(grouped_mean['note_nr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e44da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grouped_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bb74f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case of error with merging, backup of the results:\n",
    "grouped_max['note_PS_manual'] = [np.nan, 2, np.nan, 3, 1, np.nan, 1, 1, 0, 0]\n",
    "grouped_max['relevance_manual'] = [0, 1, 0, 1, 1, 0, 1, 1, 0, 0]\n",
    "grouped_max['previous_ann_PS'] = [0,1.5,1,2.5,2,0,0.5,2,0,0]\n",
    "grouped_max = grouped_max[grouped_max['relevance_manual'] == 1]\n",
    "grouped_mean['note_PS_manual'] = [np.nan, 2, np.nan, 3, 1, np.nan, 1, 1, 0, 0]\n",
    "grouped_mean['relevance_manual'] = [0, 1, 0, 1, 1, 0, 1, 1, 0, 0]\n",
    "grouped_mean = grouped_mean[grouped_mean['relevance_manual'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46fe3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grouped_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047d2a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_max['Baseline all 0'] = len(grouped_max) * [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7c4573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_test_max = pd.merge(grouped_max, total_adjucated, on='note_nr', how='left')\n",
    "#total_test_mean = pd.merge(grouped_mean, total_adjucated, on='note_nr', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dbca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_test_max.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0345a1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_adjucated[['note_nr', 'note_PS_manual']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5d0270",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_test_max[['note_nr', 'note_PS_manual']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b758049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_results['k=0'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91caa0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(total_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85562e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(bin_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64606751",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation sentence level\n",
    "bin_results.columns = bin_results.columns.str.replace('binary ', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6542096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def calculate_metrics(df, target_column, prediction_columns):\n",
    "    # Initialize a dictionary to store the results\n",
    "    results = {col: {'mae': [], 'mse': [], 'rmse': []} for col in prediction_columns}\n",
    "\n",
    "    # Loop over each fold and each prediction column\n",
    "    for fold in ['fold1', 'fold2', 'fold3', 'fold4', 'fold5']:\n",
    "        for col in prediction_columns:\n",
    "            # Filter the data for the current fold\n",
    "            fold_data = df[df['fold'] == fold]\n",
    "\n",
    "            if fold_data.empty:\n",
    "                continue\n",
    "\n",
    "            # Get the ground truth and predictions, ignoring NaN values\n",
    "            y_true = fold_data[target_column]\n",
    "            y_pred = fold_data[col].dropna()\n",
    "            y_true = y_true.loc[y_pred.index]\n",
    "\n",
    "            if y_true.empty or y_pred.empty:\n",
    "                continue\n",
    "\n",
    "            # Calculate MAE, MSE, and RMSE\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "\n",
    "            # Append the scores to the results dictionary\n",
    "            results[col]['mae'].append(mae)\n",
    "            results[col]['mse'].append(mse)\n",
    "            results[col]['rmse'].append(rmse)\n",
    "\n",
    "    return results\n",
    "\n",
    "def generate_results_table(df, target_column, prediction_columns):\n",
    "    # Calculate metrics\n",
    "    results = calculate_metrics(df, target_column, prediction_columns)\n",
    "\n",
    "    # Calculate averages and standard deviations for each prediction column\n",
    "    mae_avg_list, mae_std_list = [], []\n",
    "    mse_avg_list, mse_std_list = [], []\n",
    "    rmse_avg_list, rmse_std_list = [], []\n",
    "\n",
    "    for col in prediction_columns:\n",
    "        if results[col]['mae']:\n",
    "            mae_avg = np.mean(results[col]['mae'])\n",
    "            mae_std = np.std(results[col]['mae'])\n",
    "        else:\n",
    "            mae_avg = np.nan\n",
    "            mae_std = np.nan\n",
    "        \n",
    "        if results[col]['mse']:\n",
    "            mse_avg = np.mean(results[col]['mse'])\n",
    "            mse_std = np.std(results[col]['mse'])\n",
    "        else:\n",
    "            mse_avg = np.nan\n",
    "            mse_std = np.nan\n",
    "        \n",
    "        if results[col]['rmse']:\n",
    "            rmse_avg = np.mean(results[col]['rmse'])\n",
    "            rmse_std = np.std(results[col]['rmse'])\n",
    "        else:\n",
    "            rmse_avg = np.nan\n",
    "            rmse_std = np.nan\n",
    "\n",
    "        mae_avg_list.append(mae_avg)\n",
    "        mae_std_list.append(mae_std)\n",
    "        mse_avg_list.append(mse_avg)\n",
    "        mse_std_list.append(mse_std)\n",
    "        rmse_avg_list.append(rmse_avg)\n",
    "        rmse_std_list.append(rmse_std)\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    results_table = pd.DataFrame({\n",
    "        'Prediction Column': prediction_columns,\n",
    "        'MAE (Avg)': mae_avg_list,\n",
    "        'MAE (Std)': mae_std_list,\n",
    "        'MSE (Avg)': mse_avg_list,\n",
    "        'MSE (Std)': mse_std_list,\n",
    "        'RMSE (Avg)': rmse_avg_list,\n",
    "        'RMSE (Std)': rmse_std_list\n",
    "    })\n",
    "\n",
    "    return results_table\n",
    "\n",
    "print(\"SENTENCE LEVEL\")\n",
    "# Example of how to use the functions on a dataframe\n",
    "# Assuming `bin_results` is your dataframe and it has a column 'fold' and a target column 'relevance_manual'\n",
    "target_column = 'relevance_manual'\n",
    "prediction_columns = [f'k={i}' for i in range(0, 6)]\n",
    "results_table = generate_results_table(bin_results, 'manual_sentence_labels', ['k=0', 'k=1','k=2', 'k=3', 'k=4', 'k=5', 'Baseline all 0'])\n",
    "print(results_table.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6664c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_test_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ba50c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_test_max.columns)\n",
    "total_test_max = total_test_max[total_test_max['relevance_manual'] == 1]\n",
    "print(total_test_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1050e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def calculate_metrics(df, target_column, prediction_columns):\n",
    "    # Initialize a dictionary to store the results\n",
    "    results = {col: {'mae': [], 'mse': [], 'rmse': []} for col in prediction_columns}\n",
    "\n",
    "    # Loop over each prediction column\n",
    "    for col in prediction_columns:\n",
    "        # Get the ground truth and predictions, ignoring NaN values\n",
    "        y_true = df[target_column]\n",
    "        y_pred = df[col].dropna()\n",
    "        y_true = y_true.loc[y_pred.index]\n",
    "\n",
    "        # Calculate MAE, MSE, and RMSE\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        # Append the scores to the results dictionary\n",
    "        results[col]['mae'].append(mae)\n",
    "        results[col]['mse'].append(mse)\n",
    "        results[col]['rmse'].append(rmse)\n",
    "\n",
    "    return results\n",
    "\n",
    "def generate_results_table(df, target_column, prediction_columns):\n",
    "    # Calculate metrics\n",
    "    results = calculate_metrics(df, target_column, prediction_columns)\n",
    "\n",
    "    # Calculate averages and standard deviations for each prediction column\n",
    "    mae_avg_list, mae_std_list = [], []\n",
    "    mse_avg_list, mse_std_list = [], []\n",
    "    rmse_avg_list, rmse_std_list = [], []\n",
    "\n",
    "    for col in prediction_columns:\n",
    "        mae_avg = np.mean(results[col]['mae'])\n",
    "        mae_std = np.std(results[col]['mae'])\n",
    "        mse_avg = np.mean(results[col]['mse'])\n",
    "        mse_std = np.std(results[col]['mse'])\n",
    "        rmse_avg = np.mean(results[col]['rmse'])\n",
    "        rmse_std = np.std(results[col]['rmse'])\n",
    "\n",
    "        mae_avg_list.append(mae_avg)\n",
    "        mae_std_list.append(mae_std)\n",
    "        mse_avg_list.append(mse_avg)\n",
    "        mse_std_list.append(mse_std)\n",
    "        rmse_avg_list.append(rmse_avg)\n",
    "        rmse_std_list.append(rmse_std)\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    results_table = pd.DataFrame({\n",
    "        'Prediction Column': prediction_columns,\n",
    "        'MAE (Avg)': mae_avg_list,\n",
    "        'MAE (Std)': mae_std_list,\n",
    "        'MSE (Avg)': mse_avg_list,\n",
    "        'MSE (Std)': mse_std_list,\n",
    "        'RMSE (Avg)': rmse_avg_list,\n",
    "        'RMSE (Std)': rmse_std_list\n",
    "    })\n",
    "\n",
    "    return results_table\n",
    "\n",
    "\n",
    "print(\"NOTE LEVEL MAX\")\n",
    "# Example of how to use the functions on a dataframe\n",
    "# Assuming `bin_results` is your dataframe and it has a target column 'relevance_manual'\n",
    "prediction_columns = [f'k={i}' for i in range(0, 6)]\n",
    "results_table = generate_results_table(grouped_max, 'note_PS_manual', ['k=0', 'k=1','k=2', 'k=3', 'k=4', 'k=5', 'Baseline all 0', 'previous_ann_PS'])\n",
    "print(results_table.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d651575",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NOTE LEVEL MEAN\")\n",
    "# Example of how to use the functions on a dataframe\n",
    "# Assuming `bin_results` is your dataframe and it has a target column 'relevance_manual'\n",
    "prediction_columns = [f'k={i}' for i in range(0, 6)]\n",
    "results_table = generate_results_table(grouped_mean, 'note_PS_manual', ['k=0', 'k=1','k=2', 'k=3', 'k=4', 'k=5'])\n",
    "print(results_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae1ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_test_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85adbb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bin_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf3e5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bin_results.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206c378e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f777774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    # Initialize a dictionary to store the results\n",
    "    results = {f'k={i}': {'precision': [], 'recall': [], 'f1_score': []} for i in range(0, 6)}\n",
    "\n",
    "    # Loop over each fold and each k\n",
    "    for fold in ['fold1', 'fold2', 'fold3', 'fold4', 'fold5']:\n",
    "        for k in range(0, 6):\n",
    "            # Filter the data for the current fold\n",
    "            fold_data = df[df['fold'] == fold]\n",
    "\n",
    "            # Get the ground truth and predictions, ignoring NaN values\n",
    "            y_true = fold_data['relevance_manual']\n",
    "            y_pred = fold_data[f'k={k}'].dropna()\n",
    "            y_true = y_true[y_pred.index]\n",
    "\n",
    "            # Calculate precision, recall, and F1-score\n",
    "            precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "            # Append the scores to the results dictionary\n",
    "            results[f'k={k}']['precision'].append(precision)\n",
    "            results[f'k={k}']['recall'].append(recall)\n",
    "            results[f'k={k}']['f1_score'].append(f1)\n",
    "\n",
    "    return results\n",
    "\n",
    "def generate_results_table(df):\n",
    "    # Calculate metrics\n",
    "    results = calculate_metrics(df)\n",
    "\n",
    "    # Calculate averages and standard deviations for each k\n",
    "    precision_avg_list, precision_std_list = [], []\n",
    "    recall_avg_list, recall_std_list = [], []\n",
    "    f1_avg_list, f1_std_list = [], []\n",
    "\n",
    "    for k in range(0, 6):\n",
    "        precision_avg = np.mean(results[f'k={k}']['precision'])\n",
    "        precision_std = np.std(results[f'k={k}']['precision'])\n",
    "        recall_avg = np.mean(results[f'k={k}']['recall'])\n",
    "        recall_std = np.std(results[f'k={k}']['recall'])\n",
    "        f1_avg = np.mean(results[f'k={k}']['f1_score'])\n",
    "        f1_std = np.std(results[f'k={k}']['f1_score'])\n",
    "\n",
    "        precision_avg_list.append(precision_avg)\n",
    "        precision_std_list.append(precision_std)\n",
    "        recall_avg_list.append(recall_avg)\n",
    "        recall_std_list.append(recall_std)\n",
    "        f1_avg_list.append(f1_avg)\n",
    "        f1_std_list.append(f1_std)\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    results_table = pd.DataFrame({\n",
    "        'k': [f'k={i}' for i in range(0, 6)],\n",
    "        'Precision (Avg)': precision_avg_list,\n",
    "        'Precision (Std)': precision_std_list,\n",
    "        'Recall (Avg)': recall_avg_list,\n",
    "        'Recall (Std)': recall_std_list,\n",
    "        'F1-score (Avg)': f1_avg_list,\n",
    "        'F1-score (Std)': f1_std_list\n",
    "    })\n",
    "\n",
    "    return results_table\n",
    "\n",
    "# Example of how to use the functions on a dataframe\n",
    "print(\"ACSESS\")\n",
    "results_table1 = generate_results_table(bin_results)\n",
    "print(results_table1.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40622181",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#results_table1 = generate_results_table(bin_results, 'relevance_manual', ['k=0', 'k=1', 'k=5'])\n",
    "#print(results_table1.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e052f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_test.dropna()\n",
    "print(total_test['k=0'].unique())\n",
    "print(total_test['k=1'].unique())\n",
    "print(total_test['k=5'].unique())\n",
    "#total_test[['k=0', 'k=1', 'k=5']] = total_test[['k=0', 'k=1', 'k=5']].applymap(lambda x: 1 if x > 1 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0182cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(total_test))\n",
    "print(total_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e2a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(total_test['k=0'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02018306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_to_int(value):\n",
    "    if isinstance(value, float) and value.is_integer():\n",
    "        return int(value)\n",
    "    return value\n",
    "\n",
    "#for col in ['k=0', 'k=1', 'k=5']:\n",
    "total_test['k=0'] = total_test[col].apply(float_to_int)\n",
    "total_test['k=1'] = total_test[col].apply(float_to_int)\n",
    "total_test['k=5'] = total_test[col].apply(float_to_int)\n",
    "\n",
    "print(type(total_test['k=0'][0]))    \n",
    "    \n",
    "#for col in ['k=0', 'k=1', 'k=5']:\n",
    "#total_test = total_test[total_test[col].apply(lambda x: isinstance(x, int))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e42f1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87139727",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "print(total_test['k=0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adef69db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_metrics(df, target_column, prediction_columns):\n",
    "    # Initialize a dictionary to store the results\n",
    "    results = {col: {'precision': [], 'recall': [], 'f1_score': []} for col in prediction_columns}\n",
    "\n",
    "    # Loop over each prediction column\n",
    "    for col in prediction_columns:\n",
    "        # Get the ground truth and predictions, ignoring NaN values\n",
    "        y_true = df[target_column]\n",
    "        y_pred = df[col].dropna()\n",
    "        y_true = y_true.loc[y_pred.index]\n",
    "\n",
    "        # Calculate precision, recall, and F1-score\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "        # Append the scores to the results dictionary\n",
    "        results[col]['precision'].append(precision)\n",
    "        results[col]['recall'].append(recall)\n",
    "        results[col]['f1_score'].append(f1)\n",
    "\n",
    "    return results\n",
    "\n",
    "def generate_results_table(df, target_column, prediction_columns):\n",
    "    # Calculate metrics\n",
    "    results = calculate_metrics(df, target_column, prediction_columns)\n",
    "\n",
    "    # Calculate averages and standard deviations for each prediction column\n",
    "    precision_avg_list, precision_std_list = [], []\n",
    "    recall_avg_list, recall_std_list = [], []\n",
    "    f1_avg_list, f1_std_list = [], []\n",
    "\n",
    "    for col in prediction_columns:\n",
    "        precision_avg = np.mean(results[col]['precision'])\n",
    "        precision_std = np.std(results[col]['precision'])\n",
    "        recall_avg = np.mean(results[col]['recall'])\n",
    "        recall_std = np.std(results[col]['recall'])\n",
    "        f1_avg = np.mean(results[col]['f1_score'])\n",
    "        f1_std = np.std(results[col]['f1_score'])\n",
    "\n",
    "        precision_avg_list.append(precision_avg)\n",
    "        precision_std_list.append(precision_std)\n",
    "        recall_avg_list.append(recall_avg)\n",
    "        recall_std_list.append(recall_std)\n",
    "        f1_avg_list.append(f1_avg)\n",
    "        f1_std_list.append(f1_std)\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    results_table = pd.DataFrame({\n",
    "        'Prediction Column': prediction_columns,\n",
    "        'Precision (Avg)': precision_avg_list,\n",
    "        'Precision (Std)': precision_std_list,\n",
    "        'Recall (Avg)': recall_avg_list,\n",
    "        'Recall (Std)': recall_std_list,\n",
    "        'F1-score (Avg)': f1_avg_list,\n",
    "        'F1-score (Std)': f1_std_list\n",
    "    })\n",
    "\n",
    "    return results_table\n",
    "\n",
    "# Example of how to use the functions on a dataframe\n",
    "# Assuming `bin_results` is your dataframe and it has a target column 'relevance_manual'\n",
    "target_column = 'relevance_manual'\n",
    "prediction_columns = [f'k={i}' for i in range(0, 6)]\n",
    "results_table1 = generate_results_table(total_test, 'relevance_PS_manual', ['k=0', 'k=1', 'k=5', 'previous_ann_PS])\n",
    "print(results_table1.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbcea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_test[['relevance_PS_manual', 'k=0', 'k=1', 'k=5']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10238458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_metrics(df, target_column, prediction_columns):\n",
    "    # Initialize a dictionary to store the results\n",
    "    results = {col: {'precision': [], 'recall': [], 'f1_score': []} for col in prediction_columns}\n",
    "\n",
    "    # Loop over each prediction column\n",
    "    for col in prediction_columns:\n",
    "        # Get the ground truth and predictions, ignoring NaN values\n",
    "        y_true = df[target_column]\n",
    "        y_pred = df[col].dropna()\n",
    "        y_true = y_true.loc[y_pred.index]\n",
    "\n",
    "        # Calculate precision, recall, and F1-score\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "        # Append the scores to the results dictionary\n",
    "        results[col]['precision'].append(precision)\n",
    "        results[col]['recall'].append(recall)\n",
    "        results[col]['f1_score'].append(f1)\n",
    "\n",
    "    return results\n",
    "\n",
    "def generate_results_table(df, target_column, prediction_columns):\n",
    "    # Calculate metrics\n",
    "    results = calculate_metrics(df, target_column, prediction_columns)\n",
    "\n",
    "    # Calculate averages and standard deviations for each prediction column\n",
    "    precision_avg_list, precision_std_list = [], []\n",
    "    recall_avg_list, recall_std_list = [], []\n",
    "    f1_avg_list, f1_std_list = [], []\n",
    "\n",
    "    for col in prediction_columns:\n",
    "        precision_avg = np.mean(results[col]['precision'])\n",
    "        precision_std = np.std(results[col]['precision'])\n",
    "        recall_avg = np.mean(results[col]['recall'])\n",
    "        recall_std = np.std(results[col]['recall'])\n",
    "        f1_avg = np.mean(results[col]['f1_score'])\n",
    "        f1_std = np.std(results[col]['f1_score'])\n",
    "\n",
    "        precision_avg_list.append(precision_avg)\n",
    "        precision_std_list.append(precision_std)\n",
    "        recall_avg_list.append(recall_avg)\n",
    "        recall_std_list.append(recall_std)\n",
    "        f1_avg_list.append(f1_avg)\n",
    "        f1_std_list.append(f1_std)\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    results_table = pd.DataFrame({\n",
    "        'Prediction Column': prediction_columns,\n",
    "        'Precision (Avg)': precision_avg_list,\n",
    "        'Precision (Std)': precision_std_list,\n",
    "        'Recall (Avg)': recall_avg_list,\n",
    "        'Recall (Std)': recall_std_list,\n",
    "        'F1-score (Avg)': f1_avg_list,\n",
    "        'F1-score (Std)': f1_std_list\n",
    "    })\n",
    "\n",
    "    return results_table\n",
    "\n",
    "# Example of how to use the functions on a dataframe\n",
    "# Assuming `bin_results` is your dataframe and it has a target column 'relevance_manual'\n",
    "target_column = 'relevance_manual'\n",
    "prediction_columns = [f'k={i}' for i in range(0, 6)]\n",
    "results_table1 = generate_results_table(bin_results, 'relevance_manual', ['k=0', 'k=1', 'k=5', 'k=2'])\n",
    "print(results_table1.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074f15bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(bin_results['relevance_manual'])\n",
    "#print(bin_results['k=2'])\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "y_true = list(bin_results['relevance_manual'])\n",
    "y_pred = list(bin_results['k=2'] + 1)\n",
    "\n",
    "precision = (precision_score(y_true, y_pred))\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d56a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(bin_results['relevance_manual'][0]))\n",
    "print(type(bin_results['k=2'][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04895b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bin_results['relevance_manual']))\n",
    "print(len(bin_results['k=2']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148926c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

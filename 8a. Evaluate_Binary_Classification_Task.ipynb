{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d407953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_results_validated = pd.read_csv('test_folds_validated.csv', sep = '\\t')\n",
    "bin_results = pd.read_csv('acsess_bin_results_folds_test_2.csv', sep = '\\t')\n",
    "total_adjusted = pd.read_csv('df_total_adjucated.csv', sep = '\\t')\n",
    "fold_5 = pd.read_csv('acsess_bin_results_folds_test_fold5only.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ba0d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_results['validated'] = test_results_validated['validated']\n",
    "bin_results['binary k=5'] = fold_5['binary k=5']\n",
    "bin_results['binary k=2'] = [1] * len(bin_results)\n",
    "bin_results['binary k=3'] = [0] * len(bin_results)\n",
    "bin_results['binary k=4'] = [0] * len(bin_results)\n",
    "bin_results.columns = bin_results.columns.str.replace('binary ', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c0c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_results[['k=0', 'k=1', 'k=5']] = bin_results[['k=0', 'k=1', 'k=5']].applymap(lambda x: 1 if x > 1 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81287a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_results.dropna()\n",
    "print(bin_results.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709dc64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_results = bin_results[bin_results['relevance_manual'] == bin_results['validated']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c49e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bin_results)\n",
    "print(bin_results['note_nr'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3450ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_adjusted = total_adjusted[total_adjusted['note_nr'].isin(bin_results['note_nr'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e426a100",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_adjusted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5c6bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped = bin_results.groupby('note_nr')\n",
    "#grouped_bin = grouped.sum()\n",
    "\n",
    "grouped = bin_results.groupby('note_nr').agg({'k=0': 'max', 'k=1': 'max', 'k=5': 'max'}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee51c48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = grouped[['note_nr', 'k=0', 'k=1', 'k=5']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7c4573",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_test = pd.merge(grouped, total_adjusted, on='note_nr', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b758049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_results['k=0'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91caa0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85562e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bin_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64606751",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation sentence level\n",
    "bin_results.columns = bin_results.columns.str.replace('binary ', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f777774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    # Initialize a dictionary to store the results\n",
    "    results = {f'k={i}': {'precision': [], 'recall': [], 'f1_score': []} for i in range(0, 6)}\n",
    "\n",
    "    # Loop over each fold and each k\n",
    "    for fold in ['fold1', 'fold2', 'fold3', 'fold4', 'fold5']:\n",
    "        for k in range(0, 6):\n",
    "            # Filter the data for the current fold\n",
    "            fold_data = df[df['fold'] == fold]\n",
    "\n",
    "            # Get the ground truth and predictions, ignoring NaN values\n",
    "            y_true = fold_data['relevance_manual']\n",
    "            y_pred = fold_data[f'k={k}'].dropna()\n",
    "            y_true = y_true[y_pred.index]\n",
    "\n",
    "            # Calculate precision, recall, and F1-score\n",
    "            precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "            # Append the scores to the results dictionary\n",
    "            results[f'k={k}']['precision'].append(precision)\n",
    "            results[f'k={k}']['recall'].append(recall)\n",
    "            results[f'k={k}']['f1_score'].append(f1)\n",
    "\n",
    "    return results\n",
    "\n",
    "def generate_results_table(df):\n",
    "    # Calculate metrics\n",
    "    results = calculate_metrics(df)\n",
    "\n",
    "    # Calculate averages and standard deviations for each k\n",
    "    precision_avg_list, precision_std_list = [], []\n",
    "    recall_avg_list, recall_std_list = [], []\n",
    "    f1_avg_list, f1_std_list = [], []\n",
    "\n",
    "    for k in range(0, 6):\n",
    "        precision_avg = np.mean(results[f'k={k}']['precision'])\n",
    "        precision_std = np.std(results[f'k={k}']['precision'])\n",
    "        recall_avg = np.mean(results[f'k={k}']['recall'])\n",
    "        recall_std = np.std(results[f'k={k}']['recall'])\n",
    "        f1_avg = np.mean(results[f'k={k}']['f1_score'])\n",
    "        f1_std = np.std(results[f'k={k}']['f1_score'])\n",
    "\n",
    "        precision_avg_list.append(precision_avg)\n",
    "        precision_std_list.append(precision_std)\n",
    "        recall_avg_list.append(recall_avg)\n",
    "        recall_std_list.append(recall_std)\n",
    "        f1_avg_list.append(f1_avg)\n",
    "        f1_std_list.append(f1_std)\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    results_table = pd.DataFrame({\n",
    "        'k': [f'k={i}' for i in range(0, 6)],\n",
    "        'Precision (Avg)': precision_avg_list,\n",
    "        'Precision (Std)': precision_std_list,\n",
    "        'Recall (Avg)': recall_avg_list,\n",
    "        'Recall (Std)': recall_std_list,\n",
    "        'F1-score (Avg)': f1_avg_list,\n",
    "        'F1-score (Std)': f1_std_list\n",
    "    })\n",
    "\n",
    "    return results_table\n",
    "\n",
    "# Example of how to use the functions on a dataframe\n",
    "print(\"ACSESS\")\n",
    "results_table1 = generate_results_table(bin_results)\n",
    "print(results_table1.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40622181",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#results_table1 = generate_results_table(bin_results, 'relevance_manual', ['k=0', 'k=1', 'k=5'])\n",
    "#print(results_table1.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e052f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_test.dropna()\n",
    "print(total_test['k=0'].unique())\n",
    "print(total_test['k=1'].unique())\n",
    "print(total_test['k=5'].unique())\n",
    "#total_test[['k=0', 'k=1', 'k=5']] = total_test[['k=0', 'k=1', 'k=5']].applymap(lambda x: 1 if x > 1 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0182cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(total_test))\n",
    "print(total_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e2a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(total_test['k=0'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02018306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_to_int(value):\n",
    "    if isinstance(value, float) and value.is_integer():\n",
    "        return int(value)\n",
    "    return value\n",
    "\n",
    "#for col in ['k=0', 'k=1', 'k=5']:\n",
    "total_test['k=0'] = total_test[col].apply(float_to_int)\n",
    "total_test['k=1'] = total_test[col].apply(float_to_int)\n",
    "total_test['k=5'] = total_test[col].apply(float_to_int)\n",
    "\n",
    "print(type(total_test['k=0'][0]))    \n",
    "    \n",
    "#for col in ['k=0', 'k=1', 'k=5']:\n",
    "#total_test = total_test[total_test[col].apply(lambda x: isinstance(x, int))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e42f1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87139727",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "print(total_test['k=0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adef69db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_metrics(df, target_column, prediction_columns):\n",
    "    # Initialize a dictionary to store the results\n",
    "    results = {col: {'precision': [], 'recall': [], 'f1_score': []} for col in prediction_columns}\n",
    "\n",
    "    # Loop over each prediction column\n",
    "    for col in prediction_columns:\n",
    "        # Get the ground truth and predictions, ignoring NaN values\n",
    "        y_true = df[target_column]\n",
    "        y_pred = df[col].dropna()\n",
    "        y_true = y_true.loc[y_pred.index]\n",
    "\n",
    "        # Calculate precision, recall, and F1-score\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "        # Append the scores to the results dictionary\n",
    "        results[col]['precision'].append(precision)\n",
    "        results[col]['recall'].append(recall)\n",
    "        results[col]['f1_score'].append(f1)\n",
    "\n",
    "    return results\n",
    "\n",
    "def generate_results_table(df, target_column, prediction_columns):\n",
    "    # Calculate metrics\n",
    "    results = calculate_metrics(df, target_column, prediction_columns)\n",
    "\n",
    "    # Calculate averages and standard deviations for each prediction column\n",
    "    precision_avg_list, precision_std_list = [], []\n",
    "    recall_avg_list, recall_std_list = [], []\n",
    "    f1_avg_list, f1_std_list = [], []\n",
    "\n",
    "    for col in prediction_columns:\n",
    "        precision_avg = np.mean(results[col]['precision'])\n",
    "        precision_std = np.std(results[col]['precision'])\n",
    "        recall_avg = np.mean(results[col]['recall'])\n",
    "        recall_std = np.std(results[col]['recall'])\n",
    "        f1_avg = np.mean(results[col]['f1_score'])\n",
    "        f1_std = np.std(results[col]['f1_score'])\n",
    "\n",
    "        precision_avg_list.append(precision_avg)\n",
    "        precision_std_list.append(precision_std)\n",
    "        recall_avg_list.append(recall_avg)\n",
    "        recall_std_list.append(recall_std)\n",
    "        f1_avg_list.append(f1_avg)\n",
    "        f1_std_list.append(f1_std)\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    results_table = pd.DataFrame({\n",
    "        'Prediction Column': prediction_columns,\n",
    "        'Precision (Avg)': precision_avg_list,\n",
    "        'Precision (Std)': precision_std_list,\n",
    "        'Recall (Avg)': recall_avg_list,\n",
    "        'Recall (Std)': recall_std_list,\n",
    "        'F1-score (Avg)': f1_avg_list,\n",
    "        'F1-score (Std)': f1_std_list\n",
    "    })\n",
    "\n",
    "    return results_table\n",
    "\n",
    "# Example of how to use the functions on a dataframe\n",
    "# Assuming `bin_results` is your dataframe and it has a target column 'relevance_manual'\n",
    "target_column = 'relevance_manual'\n",
    "prediction_columns = [f'k={i}' for i in range(0, 6)]\n",
    "results_table1 = generate_results_table(total_test, 'relevance_PS_manual', ['k=0', 'k=1', 'k=5', 'previous_ann'])\n",
    "print(results_table1.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbcea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_test[['relevance_PS_manual', 'k=0', 'k=1', 'k=5']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10238458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_metrics(df, target_column, prediction_columns):\n",
    "    # Initialize a dictionary to store the results\n",
    "    results = {col: {'precision': [], 'recall': [], 'f1_score': []} for col in prediction_columns}\n",
    "\n",
    "    # Loop over each prediction column\n",
    "    for col in prediction_columns:\n",
    "        # Get the ground truth and predictions, ignoring NaN values\n",
    "        y_true = df[target_column]\n",
    "        y_pred = df[col].dropna()\n",
    "        y_true = y_true.loc[y_pred.index]\n",
    "\n",
    "        # Calculate precision, recall, and F1-score\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "        # Append the scores to the results dictionary\n",
    "        results[col]['precision'].append(precision)\n",
    "        results[col]['recall'].append(recall)\n",
    "        results[col]['f1_score'].append(f1)\n",
    "\n",
    "    return results\n",
    "\n",
    "def generate_results_table(df, target_column, prediction_columns):\n",
    "    # Calculate metrics\n",
    "    results = calculate_metrics(df, target_column, prediction_columns)\n",
    "\n",
    "    # Calculate averages and standard deviations for each prediction column\n",
    "    precision_avg_list, precision_std_list = [], []\n",
    "    recall_avg_list, recall_std_list = [], []\n",
    "    f1_avg_list, f1_std_list = [], []\n",
    "\n",
    "    for col in prediction_columns:\n",
    "        precision_avg = np.mean(results[col]['precision'])\n",
    "        precision_std = np.std(results[col]['precision'])\n",
    "        recall_avg = np.mean(results[col]['recall'])\n",
    "        recall_std = np.std(results[col]['recall'])\n",
    "        f1_avg = np.mean(results[col]['f1_score'])\n",
    "        f1_std = np.std(results[col]['f1_score'])\n",
    "\n",
    "        precision_avg_list.append(precision_avg)\n",
    "        precision_std_list.append(precision_std)\n",
    "        recall_avg_list.append(recall_avg)\n",
    "        recall_std_list.append(recall_std)\n",
    "        f1_avg_list.append(f1_avg)\n",
    "        f1_std_list.append(f1_std)\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    results_table = pd.DataFrame({\n",
    "        'Prediction Column': prediction_columns,\n",
    "        'Precision (Avg)': precision_avg_list,\n",
    "        'Precision (Std)': precision_std_list,\n",
    "        'Recall (Avg)': recall_avg_list,\n",
    "        'Recall (Std)': recall_std_list,\n",
    "        'F1-score (Avg)': f1_avg_list,\n",
    "        'F1-score (Std)': f1_std_list\n",
    "    })\n",
    "\n",
    "    return results_table\n",
    "\n",
    "# Example of how to use the functions on a dataframe\n",
    "# Assuming `bin_results` is your dataframe and it has a target column 'relevance_manual'\n",
    "target_column = 'relevance_manual'\n",
    "prediction_columns = [f'k={i}' for i in range(0, 6)]\n",
    "results_table1 = generate_results_table(bin_results, 'relevance_manual', ['k=0', 'k=1', 'k=5', 'k=2'])\n",
    "print(results_table1.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074f15bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(bin_results['relevance_manual'])\n",
    "#print(bin_results['k=2'])\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "y_true = list(bin_results['relevance_manual'])\n",
    "y_pred = list(bin_results['k=2'] + 1)\n",
    "\n",
    "precision = (precision_score(y_true, y_pred))\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d56a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(bin_results['relevance_manual'][0]))\n",
    "print(type(bin_results['k=2'][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04895b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bin_results['relevance_manual']))\n",
    "print(len(bin_results['k=2']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148926c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e18a9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00852067",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create list of all structured files (from EPIC) and list with names of files\n",
    "print('start0')\n",
    "path_to_dir=r'Z:\\final EPIC data 20220909\\*.xlsx'\n",
    "list_files=glob.glob(path_to_dir)\n",
    "data_all=[]\n",
    "file_name=[]\n",
    "print('start1')\n",
    "print(len(list_files))\n",
    "for j in range(0,len(list_files)):\n",
    "    print('j:', j)\n",
    "    data_all.append(pd.read_excel(list_files[j]))\n",
    "    file_name.append(list_files[j][32:-5])\n",
    "\n",
    "print('start2')\n",
    "##Read NKR registry data\n",
    "NKR_data=pd.read_csv(r\"Z:\\IKNL data\\IKNL data update\\K21186.csv\", delimiter=';')\n",
    "\n",
    "print('start3')\n",
    "##Read tables with mapping between Patient ID (from the structured files) to the pseudoID of the clinical notes\n",
    "AMC_conv=pd.read_excel(r\"Z:\\final EPIC data 20220909\\Text data 6-10-2022\\002a AMC CTcue import-results-2022-09-12.xlsx\")\n",
    "VUmc_conv=pd.read_excel(r\"Z:\\final EPIC data 20220909\\Text data 6-10-2022\\002b VUmc CTcue import-results-2022-09-09.xlsx\")\n",
    "\n",
    "print('start4')\n",
    "##Read clinical notes from location AMC and location VUmc\n",
    "notes_AMC=pd.read_csv(r\"Z:\\final EPIC data 20220909\\Text data 6-10-2022\\amc_data_aanvraag_20221004_aangepast.csv\", encoding=\"utf-16\")\n",
    "notes_VUmc=pd.read_csv(r\"Z:\\final EPIC data 20220909\\Text data 6-10-2022\\vumc_data_aanvraag_20221004_aangepast.csv\", encoding=\"utf-16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417658fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From clinical notes pseudo_id to Epic ID (data_all) structured data to NKR ID\n",
    "Clinical_notes_ID=notes_AMC['pseudo_id'].iloc[0] #example, Patient pseudo ID relative at clinical note at row 0, from location AMC\n",
    "EPIC_ID=AMC_conv[AMC_conv[\"Patient_pseudo_id\"]==Clinical_notes_ID].Patient_hospital_id.values[0] #ID to be used for all structured data in data_all\n",
    "NKR_ID=data_all[0][data_all[0].MDN_NKR==EPIC_ID].KEY_NKR.values[0]\n",
    "\n",
    "#From NKR ID to EPIC ID to clinical notes pseudo_id \n",
    "\n",
    "Clinical_notes_ID=notes_AMC['pseudo_id'].iloc[0] #example, Patient pseudo ID relative at clinical note at row 0, from location AMC\n",
    "\n",
    "####EPIC (structured data, Electronic Health Records): this ID is used only to map to notes and to check content of data in EPIC app (NOT RELEVANT FOR YOU!)\n",
    "EPIC_ID=AMC_conv[AMC_conv[\"Patient_pseudo_id\"]==Clinical_notes_ID].Patient_hospital_id.values[0] #ID to be used for all structured data in data_all\n",
    "\n",
    "####NKR:\n",
    "NKR_ID=data_all[0][data_all[0].MDN_NKR==EPIC_ID].KEY_NKR.values[0] #ID to be used for both NKR and for EPIC data (data_all)\n",
    "data_patient_NKR=NKR_data[NKR_data.key_nkr==NKR_ID]\n",
    "\n",
    "#Example: extract lab tests for patient with ID = NKR_ID\n",
    "data_patient_EPIC_lab=data_all[3][data_all[3].KEY_NKR==NKR_ID] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e3c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11610656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notes_VUmc['location'] = 'VUMC'\n",
    "#notes_AMC['location'] = 'AMC'\n",
    "df_amc_conv = AMC_conv\n",
    "df_vumc_conv = VUmc_conv\n",
    "notes_VUmc['location'] = 'VUmc'\n",
    "notes_AMC['location'] = 'AMC'\n",
    "df = pd.concat([notes_VUmc, notes_AMC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493690f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "df_backup = copy.deepcopy(df)\n",
    "conv = pd.concat([AMC_conv, VUmc_conv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace56878",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['note_nr'] = range(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3e36c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the DataFrames on Patient_hospital_id\n",
    "patient_id_matching = pd.merge(df_amc_conv, df_vumc_conv, on='Patient_hospital_id')\n",
    "pseudo_id_list = [item for item in df_amc_conv['Patient_pseudo_id']] + [item for item in df_vumc_conv['Patient_pseudo_id']]\n",
    "\n",
    "print(patient_id_matching.head())\n",
    "print(pseudo_id_list[:10])\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def assign_global_pseudo_id(df, patient_id_matching):\n",
    "  \"\"\"\n",
    "  Assigns a global pseudo ID to each row in 'df' based on matching in 'patient_id_matching'.\n",
    "\n",
    "  Args:\n",
    "      df (pandas.DataFrame): The DataFrame to modify.\n",
    "      patient_id_matching (pandas.DataFrame): The DataFrame containing patient ID mappings.\n",
    "\n",
    "  Returns:\n",
    "      pandas.DataFrame: The modified DataFrame with the 'global_pseudo_id' column.\n",
    "  \"\"\"\n",
    "  df['global_pseudo_id'] = df['pseudo_id'].apply(\n",
    "      lambda x: patient_id_matching[patient_id_matching['Patient_pseudo_id_x'] == x]['Patient_hospital_id'].tolist() +\n",
    "                 patient_id_matching[patient_id_matching['Patient_pseudo_id_y'] == x]['Patient_hospital_id'].tolist()\n",
    "  )\n",
    "\n",
    "  # Handle cases where no match is found (empty lists)\n",
    "  df['global_pseudo_id'] = df['global_pseudo_id'].apply(lambda x: x[0] if x else None)\n",
    "\n",
    "  return df\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "df = assign_global_pseudo_id(df.copy(), patient_id_matching.copy())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19f65ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['global_pseudo_id'].isna().sum())\n",
    "print(len(df['global_pseudo_id']) - df['global_pseudo_id'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b2db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices\n",
    "import string\n",
    "\n",
    "def anonymize_hospital_ids(df):\n",
    "  \"\"\"\n",
    "  An anonymizes hospital IDs in 'df' and generates unique pseudonyms.\n",
    "\n",
    "  Args:\n",
    "      df (pandas.DataFrame): The DataFrame containing the 'global_pseudo_id' column.\n",
    "\n",
    "  Returns:\n",
    "      tuple: A tuple containing two lists:\n",
    "          - unique_hospital_ids: List of unique hospital IDs from 'global_pseudo_id'.\n",
    "          - pseudonymised_ids: List of unique randomly generated 10-character pseudonyms.\n",
    "  \"\"\"\n",
    "  # Get unique hospital IDs\n",
    "  unique_hospital_ids = df['global_pseudo_id'].dropna().unique().tolist()\n",
    "\n",
    "  # Define function to generate random pseudonyms\n",
    "  def generate_unique_pseudonym(used_codes):\n",
    "    while True:\n",
    "      pseudonym = ''.join(choices(string.ascii_uppercase + string.digits, k=10))\n",
    "      if pseudonym not in used_codes:\n",
    "        return pseudonym\n",
    "\n",
    "  # Generate unique pseudonyms\n",
    "  used_codes = set()  # Keep track of used pseudonyms\n",
    "  pseudonymised_ids = [generate_unique_pseudonym(used_codes) for _ in range(len(unique_hospital_ids))]\n",
    "\n",
    "  # Create a dictionary for mapping (optional)\n",
    "  id_mapping = dict(zip(unique_hospital_ids, pseudonymised_ids))\n",
    "\n",
    "  return unique_hospital_ids, pseudonymised_ids, id_mapping  # Optional: return mapping dictionary\n",
    "\n",
    "# Apply the function\n",
    "unique_hospital_ids, pseudonymised_ids, id_mapping = anonymize_hospital_ids(df.copy())\n",
    "\n",
    "# Print results (optional)\n",
    "#print(f\"Unique Hospital IDs: {unique_hospital_ids}\")\n",
    "#print(f\"Pseudonymised IDs: {pseudonymised_ids}\")\n",
    "\n",
    "# You can use the id_mapping for further processing (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b17ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_pseudonyms(df, unique_hospital_ids, pseudonymised_ids):\n",
    "  \"\"\"\n",
    "  Replaces hospital IDs in 'df' with corresponding pseudonyms.\n",
    "\n",
    "  Args:\n",
    "      df (pandas.DataFrame): The DataFrame containing the 'global_pseudo_id' column.\n",
    "      unique_hospital_ids (list): List of unique hospital IDs.\n",
    "      pseudonymised_ids (list): List of pseudonyms corresponding to unique hospital IDs.\n",
    "\n",
    "  Returns:\n",
    "      pandas.DataFrame: The modified DataFrame with replaced hospital IDs.\n",
    "  \"\"\"\n",
    "\n",
    "  # Create a dictionary for mapping\n",
    "  id_mapping = dict(zip(unique_hospital_ids, pseudonymised_ids))\n",
    "\n",
    "  # Replace hospital IDs with pseudonyms using vectorized approach\n",
    "  def replace_id(x):\n",
    "    return id_mapping.get(x)  # Use get to handle potential missing values\n",
    "\n",
    "  df['global_pseudo_id'] = df['global_pseudo_id'].apply(replace_id)\n",
    "\n",
    "  return df\n",
    "\n",
    "# Apply the function\n",
    "df = replace_with_pseudonyms(df.copy(), unique_hospital_ids, pseudonymised_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a788f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to replace NaN with sequential numbers\n",
    "# def replace_nan_with_sequential(df, col_name):\n",
    "#   nan_count = 0\n",
    "#   for i in range(len(df)):\n",
    "#     if pd.isna(df.loc[i, col_name]):\n",
    "#       df.loc[i, col_name] = f\"NaN{nan_count}\"\n",
    "#       nan_count += 1\n",
    "#   return df\n",
    "\n",
    "# # Replace NaN values in the 'global_pseudo_id' column\n",
    "# df['global_pseudo_id'] = df['global_pseudo_id'].fillna(np.nan)\n",
    "# df = replace_nan_with_sequential(df.copy(), \"global_pseudo_id\")\n",
    "\n",
    "# # Print the dataframe\n",
    "# print(df.head(100))\n",
    "\n",
    "def replace_with_pseudo_id(df, col1, col2):\n",
    "    \"\"\"\n",
    "    Replaces NaN values in 'col1' with corresponding values from 'col2'.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to modify.\n",
    "        col1 (str): The column name containing NaN values.\n",
    "        col2 (str): The column name containing values to use for replacement.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The modified dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    return df.assign(**{col1: df[col1].fillna(df[col2])})\n",
    "\n",
    "# Replace NaN values\n",
    "df['global_pseudo_id'] = df['global_pseudo_id'].fillna(np.nan)\n",
    "df = replace_with_pseudo_id(df.copy(), \"global_pseudo_id\", \"pseudo_id\")\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea7106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount of notes\n",
    "print(len(df))\n",
    "# amount of patients\n",
    "print(df['global_pseudo_id'].nunique())\n",
    "print(df['pseudo_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f23123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_ids(df, pid_list):\n",
    "  \"\"\"Replaces pids in a dataframe with their corresponding global_pseudo_ids\n",
    "\n",
    "  Args:\n",
    "      df (pandas.DataFrame): The dataframe containing the pseudo_id and global_pseudo_id columns\n",
    "      pid_list (list): A list of pseudo_ids to be replaced\n",
    "\n",
    "  Returns:\n",
    "      list: A list containing the replaced global_pseudo_ids with the same length as pid_list\n",
    "  \"\"\"\n",
    "  # Create a dictionary to map pids to global_pseudo_ids\n",
    "  pid_map = dict(zip(df['pseudo_id'], df['global_pseudo_id']))\n",
    "\n",
    "  # Handle potential length mismatch\n",
    "  if len(pid_list) > len(df):\n",
    "      pid_list = pid_list[:len(df)]  # Shorten pid_list if longer\n",
    "\n",
    "  # Replace pids in the list with their corresponding global_pseudo_ids, assigning NaN for missing values\n",
    "  replaced_list = [pid_map.get(pid) for pid in pid_list]\n",
    "  replaced_list = [val if val is not None else np.NAN for val in replaced_list]\n",
    "\n",
    "  # Truncate the replaced_list to match the length of pid_list\n",
    "  replaced_list = replaced_list[:len(pid_list)]\n",
    "\n",
    "  return replaced_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d208fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb34392f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#AMC = df.loc[df['location'] == 'AMC']\n",
    "#VUMC = df.loc[df['location'] == 'VUmc']\n",
    "#AMC_100 = random.sample(AMC['pseudo_id'].unique().tolist(), 100)\n",
    "#VUMC_100 = random.sample(VUMC['pseudo_id'].unique().tolist(), 100)\n",
    "sample_50_ids = random.sample(df['pseudo_id'].unique().tolist(), 50)\n",
    "sample_50_ids = df[df['pseudo_id'].isin(sample_50_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc54a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_50_ids.reset_index(drop = True, inplace = True)\n",
    "print(sample_50_ids.head(2))\n",
    "print(sample_50_ids.columns)\n",
    "print(sample_50_ids['pseudo_id'].nunique())\n",
    "print(len(sample_50_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316927d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Clinical_notes_ID=sample_50_ids['pseudo_id'].iloc[1] #example, Patient pseudo ID relative at clinical note at row 0, from location AMC\n",
    "EPIC_ID=conv[conv[\"Patient_pseudo_id\"]==Clinical_notes_ID].Patient_hospital_id.values[0] #ID to be used for all structured data in data_all\n",
    "NKR_ID=data_all[0][data_all[0].MDN_NKR==EPIC_ID].KEY_NKR.values[0]\n",
    "\n",
    "print(Clinical_notes_ID)\n",
    "print(EPIC_ID)\n",
    "print(NKR_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640d8858",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sample_50_ids[0:10])):\n",
    "    print(sample_50_ids['pseudo_id'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf252a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NKR_list = []\n",
    "\n",
    "for i in range(len(sample_50_ids)):\n",
    "    Clinical_notes_ID=sample_50_ids['pseudo_id'].iloc[i] #example, Patient pseudo ID relative at clinical note at row 0, from location AMC\n",
    "    EPIC_ID=conv[conv[\"Patient_pseudo_id\"]==Clinical_notes_ID].Patient_hospital_id.values[0] #ID to be used for all structured data in data_all\n",
    "    NKR_ID=data_all[0][data_all[0].MDN_NKR==EPIC_ID].KEY_NKR.values[0]\n",
    "    NKR_list.append(NKR_ID)\n",
    "    \n",
    "sample_50_ids['KEY_NKR'] = NKR_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7d32d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "opname_df = data_all[29][['KEY_NKR', 'OPNAMEMOMENT', 'ONTSLAGMOMENT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f0db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_50_ids = pd.merge(sample_50_ids, opname_df, on='KEY_NKR', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c32599",
   "metadata": {},
   "outputs": [],
   "source": [
    "opname_df = opname_df[opname_df['KEY_NKR'].isin(sample_50_ids['KEY_NKR'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd86a98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opname_df['OPNAMEMOMENT'] = pd.to_datetime(opname_df['OPNAMEMOMENT'])\n",
    "opname_df['OPNAMEMOMENT'] = opname_df['OPNAMEMOMENT'].dt.date\n",
    "\n",
    "opname_df['ONTSLAGMOMENT'] = pd.to_datetime(opname_df['ONTSLAGMOMENT'])\n",
    "opname_df['ONTSLAGMOMENT'] = opname_df['ONTSLAGMOMENT'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2887fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'KEY_NKR' and aggregate the dates into lists\n",
    "result_df = opname_df.groupby('KEY_NKR').agg({\n",
    "    'OPNAMEMOMENT': lambda x: list(x),\n",
    "    'ONTSLAGMOMENT': lambda x: list(x)\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns\n",
    "result_df.columns = ['KEY_NKR', 'OPNAMEMOMENTEN', 'ONTSLAGMOMENTEN']\n",
    "\n",
    "print(result_df)\n",
    "\n",
    "opname_df = result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b60374",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_50_ids = pd.merge(sample_50_ids, opname_df, on='KEY_NKR', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad26832",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_50_ids['report_date'] = pd.to_datetime(sample_50_ids ['report_date'])\n",
    "sample_50_ids['report_date'] = sample_50_ids ['report_date'].dt.date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d0143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if report_date falls within hospital stays, with exception for same day admission and discharge\n",
    "def is_report_date_within_hospital_stay(row):\n",
    "    print(row)\n",
    "    report_date = row['report_date']\n",
    "    opname_momenten = pd.to_datetime(row['OPNAMEMOMENTEN'])\n",
    "    ontslag_momenten = pd.to_datetime(row['ONTSLAGMOMENTEN'])\n",
    "    \n",
    "    for opname, ontslag in zip(opname_momenten, ontslag_momenten):\n",
    "        if opname <= report_date <= ontslag:\n",
    "            if opname != ontslag:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Filter out rows where the report_date falls within any hospital stay period\n",
    "filtered_sample_50_ids = sample_50_ids[~sample_50_ids.apply(is_report_date_within_hospital_stay, axis=1)]\n",
    "\n",
    "# Calculate the number of same-day admission and discharge scenarios that match the report_date\n",
    "same_day_count = 0\n",
    "for index, row in sample_50_ids.iterrows():\n",
    "    report_date = row['report_date']\n",
    "    opname_momenten = pd.to_datetime(row['OPNAMEMOMENTEN'])\n",
    "    ontslag_momenten = pd.to_datetime(row['ONTSLAGMOMENTEN'])\n",
    "    for opname, ontslag in zip(opname_momenten, ontslag_momenten):\n",
    "        if opname == ontslag and opname == report_date:\n",
    "            same_day_count += 1\n",
    "\n",
    "#print(filtered_valtest)\n",
    "print(f\"Number of same-day admission and discharge scenarios matching report_date: {same_day_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcfd5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_50_ids = filtered_sample_50_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b03464",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_50_ids = sample_50_ids[sample_50_ids['report_type'].isin(['Voortgangsverslag', 'MDO-verslag', 'Anesthesie pre-op'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397ffd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_50_ids['report_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7d9e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMC = sample_50_ids.loc[sample_50_ids['location'] == 'AMC']\n",
    "VUMC = sample_50_ids.loc[sample_50_ids['location'] == 'VUmc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c0b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(AMC['report_type'].unique())\n",
    "print(VUMC['report_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828bf492",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sample_50_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f984d2",
   "metadata": {},
   "source": [
    "# Remove double ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755c6cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove from sample any double ID's\n",
    "sample_selec1 = pd.read_csv('ann1_first_round.csv')\n",
    "sample_selec2 = pd.read_csv('ann2_first_round.csv')\n",
    "sample_selec = pd.concat([sample_selec1, sample_selec2], ignore_index=True)\n",
    "\n",
    "df = df[~df['pseudo_id'].isin(sample_selec['pseudo_id'])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5044aa",
   "metadata": {},
   "source": [
    "# Remove files =< 7 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b02c62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.cli import download\n",
    "\n",
    "# Load the spaCy model for Dutch\n",
    "try:\n",
    "    nlp = spacy.load(\"nl_core_news_sm\")\n",
    "except OSError:\n",
    "    download(\"nl_core_news_sm\")\n",
    "    nlp = spacy.load(\"nl_core_news_sm\")\n",
    "    \n",
    "# Function to count sentences in a given text using spaCy\n",
    "def count_sentences(text):\n",
    "    doc = nlp(text)\n",
    "    return len(list(doc.sents))\n",
    "\n",
    "# Apply the function to the 'pseudonymized_text' column and filter rows\n",
    "df['sentence_count'] = df['pseudonomised_text'].apply(count_sentences)\n",
    "df = df[df['sentence_count'] > 7].drop(columns='sentence_count')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d87d019",
   "metadata": {},
   "source": [
    "# Remove 80% fuzzymatched files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cafee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Function to find clusters of similar strings\n",
    "def find_similar_clusters(df, threshold=80):\n",
    "    clusters = []\n",
    "    texts = df['pseudonomised_text'].tolist()\n",
    "    indices = df.index.tolist()\n",
    "\n",
    "    while texts:\n",
    "        text = texts.pop(0)\n",
    "        index = indices.pop(0)\n",
    "        cluster = [(index, text)]\n",
    "        to_remove = []\n",
    "        for other_index, other_text in zip(indices, texts):\n",
    "            if fuzz.ratio(text, other_text) >= threshold:\n",
    "                cluster.append((other_index, other_text))\n",
    "                to_remove.append((other_index, other_text))\n",
    "        for item in to_remove:\n",
    "            texts.remove(item[1])\n",
    "            indices.remove(item[0])\n",
    "        clusters.append(cluster)\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "# Function to remove shorter strings within clusters\n",
    "def remove_shorter_versions(clusters):\n",
    "    longest_indices = []\n",
    "    for cluster in clusters:\n",
    "        longest_index, longest_text = max(cluster, key=lambda x: len(x[1]))\n",
    "        longest_indices.append(longest_index)\n",
    "    return longest_indices\n",
    "\n",
    "# Find clusters of similar strings\n",
    "clusters = find_similar_clusters(df)\n",
    "\n",
    "# Remove shorter strings within each cluster\n",
    "longest_indices = remove_shorter_versions(clusters)\n",
    "\n",
    "# Create a new DataFrame with only the rows corresponding to the longest texts\n",
    "df = df.loc[longest_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076ab9c9",
   "metadata": {},
   "source": [
    "# Remove files based on (sub)tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c47f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['pseudonomised_text'].apply(lambda x: all(s in str(x).lower() for s in ['gebeld']))]\n",
    "df = df[df['pseudonomised_text'].apply(lambda x: all(s in str(x).lower() for s in ['gesproken']))]\n",
    "df = df[df['pseudonomised_text'].apply(lambda x: all(s in str(x).lower() for s in ['huisarts']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc996b91",
   "metadata": {},
   "source": [
    "# Sample val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc338fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled = []\n",
    "\n",
    "# Find the lowest number of unique pseudo-ids\n",
    "x = min(AMC['pseudo_id'].nunique(), VUMC['pseudo_id'].nunique())\n",
    "\n",
    "# Desired number of rows and tolerance\n",
    "s = 50\n",
    "tolerance = 0.05\n",
    "\n",
    "while True:\n",
    "  # Sample a number of unique pseudo-ids between 0 and x (inclusive)\n",
    "  n = random.randint(0, x)\n",
    "\n",
    "  # Sample n unique pseudo-ids from each dataframe\n",
    "  sampled_ids_amc = random.sample(AMC['pseudo_id'].unique().tolist(), n)\n",
    "  sampled_ids_vumc = random.sample(VUMC['pseudo_id'].unique().tolist(), n)\n",
    "  sampled_global_ids_amc = replace_ids(sample_50_ids.copy(), sampled_ids_amc)\n",
    "  sampled_global_ids_vumc = replace_ids(sample_50_ids.copy(), sampled_ids_vumc)\n",
    "\n",
    "   # Count occurrences in each dataframe\n",
    "  count_amc = sample_50_ids[sample_50_ids['global_pseudo_id'].isin(sampled_global_ids_amc)].shape[0]\n",
    "  count_vumc = sample_50_ids[sample_50_ids['global_pseudo_id'].isin(sampled_global_ids_vumc)].shape[0]\n",
    "  sampled_global_total = sampled_global_ids_amc + sampled_global_ids_vumc\n",
    "  count_total = sample_50_ids[sample_50_ids['global_pseudo_id'].isin(sampled_global_total)].shape[0]\n",
    "\n",
    "  # Total number of selected pseudo-ids\n",
    "  total_selected_amount = count_total\n",
    "  #print('total_selected_amount:', total_selected_amount)\n",
    "    \n",
    "  # Check if total_selected_amount is within the desired range\n",
    "  #if (total_selected_amount >= (1 - tolerance) * s) and (total_selected_amount <= (1 + tolerance) * s):\n",
    "  if (total_selected_amount >= s) and (total_selected_amount <= (1 + tolerance) * s):\n",
    "    print('total_selected_amount:', total_selected_amount)\n",
    "    break\n",
    "\n",
    "# Collect counts for each pseudo-id in each dataframe\n",
    "count_dict_amc = AMC[AMC['pseudo_id'].isin(sampled_ids_amc)]['pseudo_id'].value_counts().to_dict()\n",
    "count_dict_vumc = VUMC[VUMC['pseudo_id'].isin(sampled_ids_vumc)]['pseudo_id'].value_counts().to_dict()\n",
    "\n",
    "# Print information after reaching desired number\n",
    "print(\"VAL set:\")\n",
    "print(f\"n: {n}\")\n",
    "print(f\"count_amc: {count_amc}\")\n",
    "print(f\"count_vumc: {count_vumc}\")\n",
    "print(f\"total_selected_amount: {total_selected_amount}\")\n",
    "print(f\"count_dict_amc: {count_dict_amc}\")\n",
    "print(f\"count_dict_vumc: {count_dict_vumc}\")\n",
    "val_ids = list(count_dict_amc.keys()) + list(count_dict_vumc.keys())\n",
    "\n",
    "print(\"Reached desired number of selected pseudo-ids.\")\n",
    "\n",
    "sampled = sampled + val_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c655e49f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e42f21c1",
   "metadata": {},
   "source": [
    "# Sample test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dddd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the lowest number of unique pseudo-ids\n",
    "x = min(AMC['pseudo_id'].nunique(), VUMC['pseudo_id'].nunique())\n",
    "\n",
    "# Desired number of rows and tolerance\n",
    "s = 50\n",
    "tolerance = 0.05\n",
    "\n",
    "while True:\n",
    "  # Sample a number of unique pseudo-ids between 0 and x (inclusive)\n",
    "  n = random.randint(0, x)\n",
    "\n",
    "  # Sample n unique pseudo-ids from each dataframe\n",
    "  sampled_ids_amc = random.sample(AMC['pseudo_id'].unique().tolist(), n)\n",
    "  sampled_ids_vumc = random.sample(VUMC['pseudo_id'].unique().tolist(), n)\n",
    "  sampled_global_ids_amc = replace_ids(sample_50_ids.copy(), sampled_ids_amc)\n",
    "  sampled_global_ids_vumc = replace_ids(sample_50_ids.copy(), sampled_ids_vumc)\n",
    "\n",
    "  # Count occurrences in each dataframe\n",
    "  count_amc = sample_50_ids[sample_50_ids['global_pseudo_id'].isin(sampled_global_ids_amc)].shape[0]\n",
    "  count_vumc = sample_50_ids[sample_50_ids['global_pseudo_id'].isin(sampled_global_ids_vumc)].shape[0]\n",
    "  sampled_global_total = sampled_global_ids_amc + sampled_global_ids_vumc\n",
    "  count_total = sample_50_ids[sample_50_ids['global_pseudo_id'].isin(sampled_global_total)].shape[0]\n",
    "\n",
    "  # Total number of selected pseudo-ids\n",
    "  total_selected_amount = count_total\n",
    "  #print('total_selected_amount:', total_selected_amount)\n",
    "    \n",
    "  # Check if total_selected_amount is within the desired range\n",
    "  #if (total_selected_amount >= (1 - tolerance) * s) and (total_selected_amount <= (1 + tolerance) * s):\n",
    "  if (total_selected_amount >= s) and (total_selected_amount <= (1 + tolerance) * s):\n",
    "    print('total_selected_amount:', total_selected_amount)\n",
    "    break\n",
    "\n",
    "# Collect counts for each pseudo-id in each dataframe\n",
    "count_dict_amc = AMC[AMC['pseudo_id'].isin(sampled_ids_amc)]['pseudo_id'].value_counts().to_dict()\n",
    "count_dict_vumc = VUMC[VUMC['pseudo_id'].isin(sampled_ids_vumc)]['pseudo_id'].value_counts().to_dict()\n",
    "\n",
    "# Print information after reaching desired number\n",
    "print(\"TEST set:\")\n",
    "print(f\"n: {n}\")\n",
    "print(f\"count_amc: {count_amc}\")\n",
    "print(f\"count_vumc: {count_vumc}\")\n",
    "print(f\"total_selected_amount: {total_selected_amount}\")\n",
    "print(f\"count_dict_amc: {count_dict_amc}\")\n",
    "print(f\"count_dict_vumc: {count_dict_vumc}\")\n",
    "\n",
    "print(\"Reached desired number of selected pseudo-ids.\")\n",
    "\n",
    "test_ids = list(count_dict_amc.keys()) + list(count_dict_vumc.keys())\n",
    "\n",
    "print(\"Reached desired number of selected pseudo-ids.\")\n",
    "\n",
    "sampled = sampled + test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abcd35c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72026c6f",
   "metadata": {},
   "source": [
    "# Testing for unique global id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d029479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Global IDs val set:')\n",
    "pid_list_val = val_ids\n",
    "#replaced_list_val = replace_ids(df.copy(), pid_list_val)\n",
    "replaced_list_val = val_ids\n",
    "\n",
    "# Check if all values in the replaced list are unique\n",
    "is_unique = len(set(replaced_list_val)) == len(replaced_list_val)\n",
    "print(replaced_list_val)\n",
    "print(\"Are all values in the list unique?\", is_unique)\n",
    "\n",
    "\n",
    "print('Global IDs test set:')\n",
    "pid_list_test = test_ids\n",
    "#replaced_list_val = replace_ids(df.copy(), pid_list_val)\n",
    "replaced_list_test = test_ids\n",
    "\n",
    "\n",
    "# Check if all values in the replaced list are unique\n",
    "is_unique = len(set(replaced_list_test)) == len(replaced_list_test)\n",
    "print(replaced_list_test)\n",
    "print(\"Are all values in the list unique?\", is_unique)\n",
    "\n",
    "\n",
    "\n",
    "print('Val + test all unique?')\n",
    "# Check if all values in the replaced list are unique\n",
    "val_test_list = sampled\n",
    "is_unique = len(set(val_test_list)) == len(val_test_list)\n",
    "print(\"Are all values in the list unique?\", is_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe11dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(replaced_list_val))\n",
    "\n",
    "valcount = 0\n",
    "\n",
    "for test_id in replaced_list_val:\n",
    "    count = sample_50_ids['global_pseudo_id'].value_counts().get(test_id, 0)\n",
    "    valcount += count\n",
    "    print(test_id)\n",
    "    print(count)\n",
    "\n",
    "print(valcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79c694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(replaced_list_test))\n",
    "\n",
    "testcount = 0\n",
    "\n",
    "for test_id in replaced_list_test:\n",
    "    count = sample_50_ids['global_pseudo_id'].value_counts().get(test_id, 0)\n",
    "    testcount += count\n",
    "    print(test_id)\n",
    "    print(count)\n",
    "    \n",
    "print(testcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c27efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = sample_50_ids[sample_50_ids['global_pseudo_id'].isin(replaced_list_val)]\n",
    "test_df = sample_50_ids[sample_50_ids['global_pseudo_id'].isin(replaced_list_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9088c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_df['report_type'].unique())\n",
    "print(test_df['report_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0658d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df['set'] = 'val'\n",
    "test_df['set'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f09e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "valtest_df['report_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a00469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "valtest_df = pd.concat([val_df, test_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103520be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valtest_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ed43c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#valtest_df.to_csv('second_round_104_files.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0459e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f166da0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11ac836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

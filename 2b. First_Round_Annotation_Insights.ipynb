{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd4ecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('first_round_before_adjudicion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e6d418",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(df[df['censored'].str.contains('scopie') & df['note_PS_manual'].isna()][['censored']].head(10))\n",
    "print(len(df[df['censored'].str.contains('scopie')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c56f39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframe_by_all_strings(df, column_name1, column_name2, strings):\n",
    "    \n",
    "    if isinstance(strings, str):\n",
    "        strings = [strings]\n",
    "    \n",
    "    filtered_df = df[df[column_name1].apply(lambda x: all(s in str(x).lower() for s in strings))]\n",
    "    print(len(filtered_df))\n",
    "    nan_percentage = filtered_df[column_name2].isna().mean() \n",
    "    print(nan_percentage)\n",
    "\n",
    "print('All:')\n",
    "print(len(df))\n",
    "print(df['note_PS_manual'].isna().mean())\n",
    "print('Radio, scopie')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['scopie', 'radio'])\n",
    "print('scopie')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['scopie'])\n",
    "print('scopisch')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['scopisch'])\n",
    "print('scopie, scopisch')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['scopie', 'scopisch'])\n",
    "print('scopi')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['scopi'])\n",
    "\n",
    "\n",
    "#print('Radio')\n",
    "# filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['radio'])\n",
    "# print('Radiotherapeut')\n",
    "# filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['radiotherapeut'])\n",
    "# print('Carcinoom')\n",
    "# filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['carcinoom'])\n",
    "# print('Aanvullend onderzoek')\n",
    "# filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['aanvullend onderzoek'])\n",
    "# print('Bevindingen')\n",
    "# filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['Bevindingen'])\n",
    "print('laparoscopie')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['laparoscopie'])\n",
    "print('laparoscopisch')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['laparoscopisch'])\n",
    "print('maagcarcinoom')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['maagcarcinoom'])\n",
    "print('scopi', 'maagcarcinoom')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['scopi', 'maagcarcinoom'])\n",
    "print('gesproken')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['gesproken'])\n",
    "print('gebeld')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['gebeld'])\n",
    "print('tractus')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['tractus'])\n",
    "print('pneumonie')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['pneumonie'])\n",
    "print('bevind')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['bevind'])\n",
    "\n",
    "\n",
    "print('All:')\n",
    "print(len(df))\n",
    "print(df['note_PS_manual'].isna().mean())\n",
    "print('SCOPI')\n",
    "print('scopi')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['scopi'])\n",
    "\n",
    "print('scopi', 'maagcarcinoom')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['scopi', 'maagcarcinoom'])\n",
    "print('maagcarcinoom')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['maagcarcinoom'])\n",
    "\n",
    "print()\n",
    "\n",
    "print('scopi', 'gesproken')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['scopi', 'gesproken'])\n",
    "print('gesproken')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['gesproken'])\n",
    "\n",
    "print()\n",
    "\n",
    "print('scopi', 'gebeld')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['scopi', 'gebeld'])\n",
    "print('gebeld')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['gebeld'])\n",
    "\n",
    "print()\n",
    "\n",
    "print('scopi', 'tractus')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['scopi', 'tractus'])\n",
    "print('tractus')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['tractus'])\n",
    "\n",
    "print()\n",
    "\n",
    "print('scopi', 'pneumonie')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['scopi', 'pneumonie'])\n",
    "print('pneumonie')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['pneumonie'])\n",
    "\n",
    "print()\n",
    "\n",
    "print('scopi', 'bevind')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['scopi', 'bevind'])\n",
    "print('bevind')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['bevind'])\n",
    "\n",
    "print()\n",
    "\n",
    "print('scopi', 'laparoscopi')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['scopi', 'laparoscopi'])\n",
    "print('laparoscopi')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['laparoscopi'])\n",
    "\n",
    "print()\n",
    "\n",
    "print('scopi', 'biopt')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['scopi', 'biopt'])\n",
    "print('biopt')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['biopt'])\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "print('gesproken')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['gesproken'])\n",
    "print('gebeld')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['gebeld'])\n",
    "print('huisarts')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['huisarts'])\n",
    "print('huisarts', 'gebeld')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['huisarts', 'gebeld'])\n",
    "print('gesproken', 'gebeld')\n",
    "filter_dataframe_by_all_strings(df, 'censored', 'note_PS_manual', ['gesproken', 'gebeld'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6d4b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a28c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspellchecker\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Download the Dutch stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define the Dutch stop words\n",
    "stop_words = set(stopwords.words('dutch'))\n",
    "\n",
    "# Initialize the spellchecker for Dutch\n",
    "spell = SpellChecker(language='nl')\n",
    "\n",
    "# Function to find the most common substrings at least 5 characters long in a list of strings, excluding Dutch stop words\n",
    "def find_most_common_substrings(notes, min_length=5, n=20):\n",
    "    # Combine all notes into a single text\n",
    "    combined_text = ' '.join(notes).lower()\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words_list = re.findall(r'\\b\\w+\\b', combined_text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    filtered_words = [word for word in words_list if word not in stop_words]\n",
    "    \n",
    "    # Collect all substrings at least min_length long\n",
    "    substrings = []\n",
    "    for word in filtered_words:\n",
    "        length = len(word)\n",
    "        for i in range(length):\n",
    "            for j in range(i + min_length, length + 1):\n",
    "                substrings.append(word[i:j])\n",
    "    \n",
    "    # Count the frequency of each substring\n",
    "    substring_counts = Counter(substrings)\n",
    "    \n",
    "    # Find the most common substrings\n",
    "    most_common_substrings = substring_counts.most_common()\n",
    "    \n",
    "    # Filter substrings to include only those found in the Dutch dictionary\n",
    "    filtered_common_substrings = [\n",
    "        (substring, count) for substring, count in most_common_substrings\n",
    "        if spell.correction(substring) == substring\n",
    "    ]\n",
    "    \n",
    "    return filtered_common_substrings[:n]\n",
    "\n",
    "notes = df[df['censored'].apply(lambda x: all(s in str(x).lower() for s in ['scopie']))]['censored']\n",
    "\n",
    "most_common_substrings = find_most_common_substrings(notes, min_length=5, n=20)\n",
    "print(\"Most common substrings (at least 5 characters, in Dutch dictionary, excluding stop words):\")\n",
    "for substring, freq in most_common_substrings:\n",
    "    print(f\"{substring}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e621b91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62b8168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from scipy.stats import fisher_exact\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Download the Dutch stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define the Dutch stop words\n",
    "stop_words = set(stopwords.words('dutch'))\n",
    "\n",
    "# Initialize the spellchecker for Dutch\n",
    "spell = SpellChecker(language='nl')\n",
    "\n",
    "# Function to find valid Dutch subtokens of at least min_length in the notes\n",
    "def get_subtokens(words_list, min_length):\n",
    "    valid_subtokens = []\n",
    "    for word in words_list:\n",
    "        if len(word) >= min_length and word not in stop_words:\n",
    "            for i in range(len(word) - min_length + 1):\n",
    "                for j in range(i + min_length, len(word) + 1):\n",
    "                    subtoken = word[i:j]\n",
    "                    if subtoken not in stop_words:\n",
    "                        valid_subtokens.append(subtoken)\n",
    "    return valid_subtokens\n",
    "\n",
    "# Function to perform Fisher's exact test to find statistically significant words\n",
    "def find_significant_words(list1, list2, min_length=5, alpha=0.10, min_occurrences=10):\n",
    "    combined_text1 = ' '.join(list1).lower()\n",
    "    combined_text2 = ' '.join(list2).lower()\n",
    "    \n",
    "    words_list1 = re.findall(r'\\b\\w+\\b', combined_text1)\n",
    "    words_list2 = re.findall(r'\\b\\w+\\b', combined_text2)\n",
    "    \n",
    "    subtokens_list1 = get_subtokens(words_list1, min_length)\n",
    "    subtokens_list2 = get_subtokens(words_list2, min_length)\n",
    "    \n",
    "    counter1 = Counter(subtokens_list1)\n",
    "    counter2 = Counter(subtokens_list2)\n",
    "    \n",
    "    # Filter to include only subtokens with a total occurrence of at least min_occurrences across both lists\n",
    "    all_subtokens = {subtoken for subtoken in set(counter1.keys()).union(set(counter2.keys()))\n",
    "                     if counter1.get(subtoken, 0) + counter2.get(subtoken, 0) >= min_occurrences}\n",
    "    \n",
    "    significant_words_list1 = []\n",
    "    significant_words_list2 = []\n",
    "    for subtoken in all_subtokens:\n",
    "        count1 = counter1.get(subtoken, 0)\n",
    "        count2 = counter2.get(subtoken, 0)\n",
    "        # Construct the contingency table\n",
    "        table = [[count1, len(subtokens_list1) - count1], [count2, len(subtokens_list2) - count2]]\n",
    "        \n",
    "        # Perform Fisher's exact test\n",
    "        _, p = fisher_exact(table, alternative='greater')  # Test if the subtoken is more common in list1\n",
    "        if p < alpha:\n",
    "            if count1 > count2:\n",
    "                significant_words_list1.append((subtoken, count1, count2, p))\n",
    "        \n",
    "        _, p = fisher_exact(table, alternative='less')  # Test if the subtoken is more common in list2\n",
    "        if p < alpha:\n",
    "            if count2 > count1:\n",
    "                significant_words_list2.append((subtoken, count1, count2, p))\n",
    "    \n",
    "    return (\n",
    "        sorted(significant_words_list1, key=lambda x: x[3]),\n",
    "        sorted(significant_words_list2, key=lambda x: x[3])\n",
    "    )\n",
    "\n",
    "# Filter function to keep only words present in the Dutch dictionary\n",
    "def filter_valid_words(words_list):\n",
    "    return [word for word in words_list if spell.correction(word) == word]\n",
    "\n",
    "#list1 = df[df['censored'].apply(lambda x: all(s in str(x).lower() for s in ['scopie']))]['censored']\n",
    "list1 = df[df['note_PS_manual'].notna()]\n",
    "#list2 = df[~df['censored'].apply(lambda x: all(s in str(x).lower() for s in ['scopie']))]['censored']\n",
    "list2 = df[df['note_PS_manual'].isna()]\n",
    "\n",
    "# Parameters\n",
    "pvalue = 0.30\n",
    "min_occurrences = 25\n",
    "min_length = 5\n",
    "\n",
    "# Find significant words\n",
    "significant_words_list1, significant_words_list2 = find_significant_words(list1, list2, min_length=min_length, alpha=pvalue, min_occurrences=min_occurrences)\n",
    "\n",
    "# Filter significant words to include only those in the Dutch dictionary\n",
    "filtered_significant_words_list1 = [(word, count1, count2, p) for word, count1, count2, p in significant_words_list1 if spell.correction(word) == word]\n",
    "filtered_significant_words_list2 = [(word, count1, count2, p) for word, count1, count2, p in significant_words_list2 if spell.correction(word) == word]\n",
    "\n",
    "# Print results\n",
    "print(\"Statistically significant words (at least 5 characters) appearing more in list1 than list2:\")\n",
    "for word, count1, count2, p in significant_words_list1:\n",
    "    print(f\"{word}: {count1} (list1), {count2} (list2), p-value: {p:.5f}\")\n",
    "\n",
    "print(\"\\nStatistically significant words (at least 5 characters) appearing more in list2 than list1:\")\n",
    "for word, count1, count2, p in significant_words_list2:\n",
    "    print(f\"{word}: {count1} (list1), {count2} (list2), p-value: {p:.5f}\")\n",
    "\n",
    "print(\"\\nFiltered statistically significant words (at least 5 characters) appearing more in list1 than list2 (valid Dutch words):\")\n",
    "for word, count1, count2, p in filtered_significant_words_list1:\n",
    "    print(f\"{word}: {count1} (list1), {count2} (list2), p-value: {p:.5f}\")\n",
    "\n",
    "print(\"\\nFiltered statistically significant words (at least 5 characters) appearing more in list2 than list1 (valid Dutch words):\")\n",
    "for word, count1, count2, p in filtered_significant_words_list2:\n",
    "    print(f\"{word}: {count1} (list1), {count2} (list2), p-value: {p:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff755817",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[df['censored'].apply(lambda x: all(s in str(x).lower() for s in ['scopie']))]['censored']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdec96a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from scipy.stats import fisher_exact\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Download the Dutch stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define the Dutch stop words\n",
    "stop_words = set(stopwords.words('dutch'))\n",
    "\n",
    "# Initialize the spellchecker for Dutch\n",
    "spell = SpellChecker(language='nl')\n",
    "\n",
    "# Function to find valid Dutch subtokens of at least min_length in the notes\n",
    "def get_subtokens(words_list, min_length):\n",
    "    valid_subtokens = []\n",
    "    for word in words_list:\n",
    "        if len(word) >= min_length and word not in stop_words:\n",
    "            for i in range(len(word) - min_length + 1):\n",
    "                for j in range(i + min_length, len(word) + 1):\n",
    "                    subtoken = word[i:j]\n",
    "                    if subtoken not in stop_words:\n",
    "                        valid_subtokens.append(subtoken)\n",
    "    return valid_subtokens\n",
    "\n",
    "# Function to process notes and extract subtokens\n",
    "def process_notes(notes, min_length):\n",
    "    combined_text = ' '.join(notes).lower()\n",
    "    words_list = re.findall(r'\\b\\w+\\b', combined_text)\n",
    "    return get_subtokens(words_list, min_length)\n",
    "\n",
    "# Function to perform Fisher's exact test to find statistically significant words\n",
    "def find_significant_words(list1, list2, min_length=7, alpha=0.10, min_occurrences=25):\n",
    "    subtokens_list1 = process_notes(list1, min_length)\n",
    "    subtokens_list2 = process_notes(list2, min_length)\n",
    "    \n",
    "    counter1 = Counter(subtokens_list1)\n",
    "    counter2 = Counter(subtokens_list2)\n",
    "    \n",
    "    # Filter to include only subtokens with a total occurrence of at least min_occurrences across both lists\n",
    "    all_subtokens = {subtoken for subtoken in set(counter1.keys()).union(set(counter2.keys()))\n",
    "                     if counter1.get(subtoken, 0) + counter2.get(subtoken, 0) >= min_occurrences}\n",
    "    \n",
    "    significant_words_list1 = []\n",
    "    significant_words_list2 = []\n",
    "    total_subtokens = len(all_subtokens)\n",
    "    progress_intervals = total_subtokens // 10\n",
    "    \n",
    "    for i, subtoken in enumerate(all_subtokens):\n",
    "        count1 = counter1.get(subtoken, 0)\n",
    "        count2 = counter2.get(subtoken, 0)\n",
    "        # Construct the contingency table\n",
    "        table = [[count1, len(subtokens_list1) - count1], [count2, len(subtokens_list2) - count2]]\n",
    "        \n",
    "        # Perform Fisher's exact test\n",
    "        _, p = fisher_exact(table, alternative='greater')  # Test if the subtoken is more common in list1\n",
    "        if p < alpha:\n",
    "            if count1 > count2:\n",
    "                significant_words_list1.append((subtoken, count1, count2, p))\n",
    "        \n",
    "        _, p = fisher_exact(table, alternative='less')  # Test if the subtoken is more common in list2\n",
    "        if p < alpha:\n",
    "            if count2 > count1:\n",
    "                significant_words_list2.append((subtoken, count1, count2, p))\n",
    "        \n",
    "        # Print progress\n",
    "        if progress_intervals > 0 and (i + 1) % progress_intervals == 0:\n",
    "            print(f\"Progress: {((i + 1) // progress_intervals) * 10}%\")\n",
    "    \n",
    "    return (\n",
    "        sorted(significant_words_list1, key=lambda x: x[3]),\n",
    "        sorted(significant_words_list2, key=lambda x: x[3])\n",
    "    )\n",
    "\n",
    "# Filter function to keep only words present in the Dutch dictionary\n",
    "def filter_valid_words(words_list):\n",
    "    return [word for word in words_list if spell.correction(word) == word]\n",
    "\n",
    "#list1 = df[df['censored'].apply(lambda x: all(s in str(x).lower() for s in ['scopie']))]['censored']\n",
    "list1 = df[df['note_PS_manual'].notna()]['censored']\n",
    "#list2 = df[~df['censored'].apply(lambda x: all(s in str(x).lower() for s in ['scopie']))]['censored']\n",
    "list2 = df[df['note_PS_manual'].isna()]['censored']\n",
    "\n",
    "# Parameters\n",
    "pvalue = 0.30  # Adjusted to a more common significance level\n",
    "min_occurrences = 20  # Increased for faster execution\n",
    "min_length = 5  # Increased for faster execution\n",
    "\n",
    "# Find significant words\n",
    "significant_words_list1, significant_words_list2 = find_significant_words(list1, list2, min_length=min_length, alpha=pvalue, min_occurrences=min_occurrences)\n",
    "\n",
    "# Filter significant words to include only those in the Dutch dictionary\n",
    "filtered_significant_words_list1 = [(word, count1, count2, p) for word, count1, count2, p in significant_words_list1 if spell.correction(word) == word]\n",
    "filtered_significant_words_list2 = [(word, count1, count2, p) for word, count1, count2, p in significant_words_list2 if spell.correction(word) == word]\n",
    "\n",
    "# Print results\n",
    "print(\"Statistically significant words (at least 7 characters) appearing more in list1 than list2:\")\n",
    "for word, count1, count2, p in significant_words_list1:\n",
    "    print(f\"{word}: {count1} (list1), {count2} (list2), p-value: {p:.5f}\")\n",
    "\n",
    "print(\"\\nStatistically significant words (at least 7 characters) appearing more in list2 than list1:\")\n",
    "for word, count1, count2, p in significant_words_list2:\n",
    "    print(f\"{word}: {count1} (list1), {count2} (list2), p-value: {p:.5f}\")\n",
    "\n",
    "print(\"\\nFiltered statistically significant words (at least 7 characters) appearing more in list1 than list2 (valid Dutch words):\")\n",
    "for word, count1, count2, p in filtered_significant_words_list1:\n",
    "    print(f\"{word}: {count1} (list1), {count2} (list2), p-value: {p:.5f}\")\n",
    "\n",
    "print(\"\\nFiltered statistically significant words (at least 7 characters) appearing more in list2 than list1 (valid Dutch words):\")\n",
    "for word, count1, count2, p in filtered_significant_words_list2:\n",
    "    print(f\"{word}: {count1} (list1), {count2} (list2), p-value: {p:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88c91e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from scipy.stats import fisher_exact\n",
    "from spellchecker import SpellChecker\n",
    "import time\n",
    "\n",
    "# Download the Dutch stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define the Dutch stop words\n",
    "stop_words = set(stopwords.words('dutch'))\n",
    "\n",
    "# Initialize the spellchecker for Dutch\n",
    "spell = SpellChecker(language='nl')\n",
    "\n",
    "# Function to find valid Dutch subtokens of at least min_length in the notes\n",
    "def get_subtokens(words_list, min_length):\n",
    "    valid_subtokens = []\n",
    "    for word in words_list:\n",
    "        if len(word) >= min_length and word not in stop_words:\n",
    "            for i in range(len(word) - min_length + 1):\n",
    "                for j in range(i + min_length, len(word) + 1):\n",
    "                    subtoken = word[i:j]\n",
    "                    if subtoken not in stop_words:\n",
    "                        valid_subtokens.append(subtoken)\n",
    "    return valid_subtokens\n",
    "\n",
    "# Function to process notes and extract subtokens\n",
    "def process_notes(notes, min_length):\n",
    "    combined_text = ' '.join(notes).lower()\n",
    "    words_list = re.findall(r'\\b\\w+\\b', combined_text)\n",
    "    return get_subtokens(words_list, min_length)\n",
    "\n",
    "# Function to perform Fisher's exact test to find statistically significant words\n",
    "def find_significant_words(list1, list2, min_length=7, alpha=0.10, min_occurrences=25):\n",
    "    print(\"Starting subtoken processing...\")\n",
    "    subtokens_list1 = process_notes(list1, min_length)\n",
    "    subtokens_list2 = process_notes(list2, min_length)\n",
    "    print(\"Subtoken processing completed.\")\n",
    "    \n",
    "    print(\"Counting subtokens...\")\n",
    "    counter1 = Counter(subtokens_list1)\n",
    "    counter2 = Counter(subtokens_list2)\n",
    "    print(\"Subtoken counting completed.\")\n",
    "    \n",
    "    print(\"Filtering subtokens...\")\n",
    "    all_subtokens = {subtoken for subtoken in set(counter1.keys()).union(set(counter2.keys()))\n",
    "                     if counter1.get(subtoken, 0) + counter2.get(subtoken, 0) >= min_occurrences}\n",
    "    print(\"Subtoken filtering completed.\")\n",
    "    \n",
    "    significant_words_list1 = []\n",
    "    significant_words_list2 = []\n",
    "    total_subtokens = len(all_subtokens)\n",
    "    progress_intervals = total_subtokens // 10\n",
    "    \n",
    "    print(\"Starting statistical testing...\")\n",
    "    for i, subtoken in enumerate(all_subtokens):\n",
    "        count1 = counter1.get(subtoken, 0)\n",
    "        count2 = counter2.get(subtoken, 0)\n",
    "        # Construct the contingency table\n",
    "        table = [[count1, len(subtokens_list1) - count1], [count2, len(subtokens_list2) - count2]]\n",
    "        \n",
    "        # Perform Fisher's exact test\n",
    "        _, p = fisher_exact(table, alternative='greater')  # Test if the subtoken is more common in list1\n",
    "        if p < alpha:\n",
    "            if count1 > count2:\n",
    "                significant_words_list1.append((subtoken, count1, count2, p))\n",
    "        \n",
    "        _, p = fisher_exact(table, alternative='less')  # Test if the subtoken is more common in list2\n",
    "        if p < alpha:\n",
    "            if count2 > count1:\n",
    "                significant_words_list2.append((subtoken, count1, count2, p))\n",
    "        \n",
    "        # Print progress\n",
    "        if progress_intervals > 0 and (i + 1) % progress_intervals == 0:\n",
    "            print(f\"Progress: {((i + 1) // progress_intervals) * 10}%\")\n",
    "    print(\"Statistical testing completed.\")\n",
    "    \n",
    "    print(\"Sorting significant words...\")\n",
    "    sorted_significant_words_list1 = sorted(significant_words_list1, key=lambda x: x[3])\n",
    "    sorted_significant_words_list2 = sorted(significant_words_list2, key=lambda x: x[3])\n",
    "    print(\"Sorting completed.\")\n",
    "    \n",
    "    #print(\"Filtering valid Dutch words...\")\n",
    "    #filtered_significant_words_list1 = [(word, count1, count2, p) for word, count1, count2, p in sorted_significant_words_list1 if spell.correction(word) == word]\n",
    "    #filtered_significant_words_list2 = [(word, count1, count2, p) for word, count1, count2, p in sorted_significant_words_list2 if spell.correction(word) == word]\n",
    "    #print(\"Filtering completed.\")\n",
    "    \n",
    "    filtered_significant_words_list1 = sorted_significant_words_list1\n",
    "    filtered_significant_words_list2 = sorted_significant_words_list2\n",
    "    \n",
    "    return filtered_significant_words_list1, filtered_significant_words_list2\n",
    "\n",
    "#list1 = df[df['censored'].apply(lambda x: all(s in str(x).lower() for s in ['scopie']))]['censored']\n",
    "list1 = df[df['note_PS_manual'].notna()]['censored']\n",
    "#list2 = df[~df['censored'].apply(lambda x: all(s in str(x).lower() for s in ['scopie']))]['censored']\n",
    "list2 = df[df['note_PS_manual'].isna()]['censored']\n",
    "\n",
    "# Parameters\n",
    "pvalue = 0.20  # Adjusted to a more common significance level\n",
    "min_occurrences = 25  # Increased for faster execution\n",
    "min_length = 7  # Increased for faster execution\n",
    "\n",
    "# Measure the execution time\n",
    "start_time = time.time()\n",
    "significant_words_list1, significant_words_list2 = find_significant_words(list1, list2, min_length=min_length, alpha=pvalue, min_occurrences=min_occurrences)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Execution time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Print results\n",
    "print(\"Statistically significant words (at least 7 characters) appearing more in list1 than list2:\")\n",
    "for word, count1, count2, p in significant_words_list1:\n",
    "    print(f\"{word}: {count1} (list1), {count2} (list2), p-value: {p:.5f}\")\n",
    "\n",
    "print(\"\\nStatistically significant words (at least 7 characters) appearing more in list2 than list1:\")\n",
    "for word, count1, count2, p in significant_words_list2:\n",
    "    print(f\"{word}: {count1} (list1), {count2} (list2), p-value: {p:.5f}\")\n",
    "\n",
    "# Filter significant words to include only those in the Dutch dictionary\n",
    "filtered_significant_words_list1 = [(word, count1, count2, p) for word, count1, count2, p in significant_words_list1 if spell.correction(word) == word]\n",
    "filtered_significant_words_list2 = [(word, count1, count2, p) for word, count1, count2, p in significant_words_list2 if spell.correction(word) == word]\n",
    "\n",
    "print(\"\\nFiltered statistically significant words (at least 7 characters) appearing more in list1 than list2 (valid Dutch words):\")\n",
    "for word, count1, count2, p in filtered_significant_words_list1:\n",
    "    print(f\"{word}: {count1} (list1), {count2} (list2), p-value: {p:.5f}\")\n",
    "\n",
    "print(\"\\nFiltered statistically significant words (at least 7 characters) appearing more in list2 than list1 (valid Dutch words):\")\n",
    "for word, count1, count2, p in filtered_significant_words_list2:\n",
    "    print(f\"{word}: {count1} (list1), {count2} (list2), p-value: {p:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092976f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from scipy.stats import fisher_exact\n",
    "\n",
    "# Download the Dutch stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define the Dutch stop words\n",
    "stop_words = set(stopwords.words('dutch'))\n",
    "\n",
    "# Function to find valid Dutch subtokens of at least min_length in the notes\n",
    "def get_subtokens(words_list, min_length):\n",
    "    valid_subtokens = []\n",
    "    for word in words_list:\n",
    "        if len(word) >= min_length and word not in stop_words:\n",
    "            for i in range(len(word) - min_length + 1):\n",
    "                for j in range(i + min_length, len(word) + 1):\n",
    "                    subtoken = word[i:j]\n",
    "                    if subtoken not in stop_words:\n",
    "                        valid_subtokens.append(subtoken)\n",
    "    return valid_subtokens\n",
    "\n",
    "# Function to process notes and extract subtokens\n",
    "def process_notes(notes, min_length):\n",
    "    combined_text = ' '.join(notes).lower()\n",
    "    words_list = re.findall(r'\\b\\w+\\b', combined_text)\n",
    "    return get_subtokens(words_list, min_length)\n",
    "\n",
    "# Function to perform Fisher's exact test to find statistically significant words\n",
    "def find_significant_words(list1, list2, min_length=7, alpha=0.10, min_occurrences=25):\n",
    "    subtokens_list1 = process_notes(list1, min_length)\n",
    "    subtokens_list2 = process_notes(list2, min_length)\n",
    "    \n",
    "    counter1 = Counter(subtokens_list1)\n",
    "    counter2 = Counter(subtokens_list2)\n",
    "    \n",
    "    # Filter to include only subtokens with a total occurrence of at least min_occurrences across both lists\n",
    "    all_subtokens = {subtoken for subtoken in set(counter1.keys()).union(set(counter2.keys()))\n",
    "                     if counter1.get(subtoken, 0) + counter2.get(subtoken, 0) >= min_occurrences}\n",
    "    \n",
    "    significant_words_list1 = []\n",
    "    significant_words_list2 = []\n",
    "    total_subtokens = len(all_subtokens)\n",
    "    progress_intervals = total_subtokens // 10\n",
    "    \n",
    "    for i, subtoken in enumerate(all_subtokens):\n",
    "        count1 = counter1.get(subtoken, 0)\n",
    "        count2 = counter2.get(subtoken, 0)\n",
    "        # Construct the contingency table\n",
    "        table = [[count1, len(subtokens_list1) - count1], [count2, len(subtokens_list2) - count2]]\n",
    "        \n",
    "        # Perform Fisher's exact test\n",
    "        _, p = fisher_exact(table, alternative='greater')  # Test if the subtoken is more common in list1\n",
    "        if p < alpha:\n",
    "            if count1 > count2:\n",
    "                significant_words_list1.append((subtoken, count1, count2, p))\n",
    "        \n",
    "        _, p = fisher_exact(table, alternative='less')  # Test if the subtoken is more common in list2\n",
    "        if p < alpha:\n",
    "            if count2 > count1:\n",
    "                significant_words_list2.append((subtoken, count1, count2, p))\n",
    "        \n",
    "        # Print progress\n",
    "        if progress_intervals > 0 and (i + 1) % progress_intervals == 0:\n",
    "            print(f\"Progress: {((i + 1) // progress_intervals) * 10}%\")\n",
    "    \n",
    "    return (\n",
    "        sorted(significant_words_list1, key=lambda x: x[3]),\n",
    "        sorted(significant_words_list2, key=lambda x: x[3])\n",
    "    )\n",
    "\n",
    "#list1 = df[df['censored'].apply(lambda x: all(s in str(x).lower() for s in ['scopie']))]['censored']\n",
    "list1 = df[df['note_PS_manual'].notna()]['censored']\n",
    "#list2 = df[~df['censored'].apply(lambda x: all(s in str(x).lower() for s in ['scopie']))]['censored']\n",
    "list2 = df[df['note_PS_manual'].isna()]['censored']\n",
    "\n",
    "# Parameters\n",
    "pvalue = 0.20  # Adjusted to a more common significance level\n",
    "min_occurrences = 20  # Increased for faster execution\n",
    "min_length = 5  # Increased for faster execution\n",
    "\n",
    "# Find significant words\n",
    "significant_words_list1, significant_words_list2 = find_significant_words(list1, list2, min_length=min_length, alpha=pvalue, min_occurrences=min_occurrences)\n",
    "\n",
    "# Print results\n",
    "print(\"Statistically significant words (at least 7 characters) appearing more in list1 than list2:\")\n",
    "for word, count1, count2, p in significant_words_list1:\n",
    "    print(f\"{word}: {count1} (list1), {count2} (list2), p-value: {p:.5f}\")\n",
    "\n",
    "print(\"\\nStatistically significant words (at least 7 characters) appearing more in list2 than list1:\")\n",
    "for word, count1, count2, p in significant_words_list2:\n",
    "    print(f\"{word}: {count1} (list1), {count2} (list2), p-value: {p:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f9b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from scipy.stats import fisher_exact\n",
    "\n",
    "# Download the Dutch stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define the Dutch stop words\n",
    "stop_words = set(stopwords.words('dutch'))\n",
    "\n",
    "# Function to find valid Dutch subtokens of at least min_length in the notes\n",
    "def get_subtokens(words_list, min_length):\n",
    "    valid_subtokens = []\n",
    "    for word in words_list:\n",
    "        if len(word) >= min_length and word not in stop_words:\n",
    "            for i in range(len(word) - min_length + 1):\n",
    "                for j in range(i + min_length, len(word) + 1):\n",
    "                    subtoken = word[i:j]\n",
    "                    if subtoken not in stop_words:\n",
    "                        valid_subtokens.append(subtoken)\n",
    "    return valid_subtokens\n",
    "\n",
    "# Function to process notes and extract subtokens\n",
    "def process_notes(notes, min_length):\n",
    "    combined_text = ' '.join(notes).lower()\n",
    "    words_list = re.findall(r'\\b\\w+\\b', combined_text)\n",
    "    return get_subtokens(words_list, min_length)\n",
    "\n",
    "# Function to perform Fisher's exact test to find statistically significant words\n",
    "def find_significant_words(list1, list2, min_length=7, alpha=0.10, min_occurrences=25):\n",
    "    subtokens_list1 = process_notes(list1, min_length)\n",
    "    subtokens_list2 = process_notes(list2, min_length)\n",
    "    \n",
    "    counter1 = Counter(subtokens_list1)\n",
    "    counter2 = Counter(subtokens_list2)\n",
    "    \n",
    "    # Filter to include only subtokens with a total occurrence of at least min_occurrences across both lists\n",
    "    all_subtokens = {subtoken for subtoken in set(counter1.keys()).union(set(counter2.keys()))\n",
    "                     if counter1.get(subtoken, 0) + counter2.get(subtoken, 0) >= min_occurrences}\n",
    "    \n",
    "    significant_words_list1 = []\n",
    "    significant_words_list2 = []\n",
    "    total_subtokens = len(all_subtokens)\n",
    "    progress_intervals = total_subtokens // 10\n",
    "    \n",
    "    for i, subtoken in enumerate(all_subtokens):\n",
    "        count1 = counter1.get(subtoken, 0)\n",
    "        count2 = counter2.get(subtoken, 0)\n",
    "        # Construct the contingency table\n",
    "        table = [[count1, len(subtokens_list1) - count1], [count2, len(subtokens_list2) - count2]]\n",
    "        \n",
    "        # Perform Fisher's exact test\n",
    "        _, p = fisher_exact(table, alternative='greater')  # Test if the subtoken is more common in list1\n",
    "        if p < alpha:\n",
    "            if count1 > count2:\n",
    "                significant_words_list1.append((subtoken, count1, count2, p))\n",
    "        \n",
    "        _, p = fisher_exact(table, alternative='less')  # Test if the subtoken is more common in list2\n",
    "        if p < alpha:\n",
    "            if count2 > count1:\n",
    "                significant_words_list2.append((subtoken, count1, count2, p))\n",
    "        \n",
    "        # Print progress\n",
    "        if progress_intervals > 0 and (i + 1) % progress_intervals == 0:\n",
    "            print(f\"Progress: {((i + 1) // progress_intervals) * 10}%\")\n",
    "    \n",
    "    return (\n",
    "        sorted(significant_words_list1, key=lambda x: x[3]),\n",
    "        sorted(significant_words_list2, key=lambda x: x[3])\n",
    "    )\n",
    "\n",
    "#list1 = df[df['censored'].apply(lambda x: all(s in str(x).lower() for s in ['scopie']))]['censored']\n",
    "list1 = df[df['censored'].apply(lambda x: all(s in str(x).lower() for s in ['scopi']))]['censored']\n",
    "#list2 = df[~df['censored'].apply(lambda x: all(s in str(x).lower() for s in ['scopie']))]['censored']\n",
    "list2 = df[~df['censored'].apply(lambda x: all(s in str(x).lower() for s in ['scopi']))]['censored']\n",
    "\n",
    "\n",
    "# Parameters\n",
    "pvalue = 0.20  # Adjusted to a more common significance level\n",
    "min_occurrences = 20  # Increased for faster execution\n",
    "min_length = 5  # Increased for faster execution\n",
    "\n",
    "# Find significant words\n",
    "significant_words_list1, significant_words_list2 = find_significant_words(list1, list2, min_length=min_length, alpha=pvalue, min_occurrences=min_occurrences)\n",
    "\n",
    "# Print results\n",
    "print(\"Statistically significant words (at least 7 characters) appearing more in list1 than list2:\")\n",
    "for word, count1, count2, p in significant_words_list1:\n",
    "    print(f\"{word}: {count1} (list1), {count2} (list2), p-value: {p:.5f}\")\n",
    "\n",
    "print(\"\\nStatistically significant words (at least 7 characters) appearing more in list2 than list1:\")\n",
    "for word, count1, count2, p in significant_words_list2:\n",
    "    print(f\"{word}: {count1} (list1), {count2} (list2), p-value: {p:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cbe1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a54c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to count sentences in a list of sentences\n",
    "def count_sentences(sentences):\n",
    "    return len(sentences)\n",
    "\n",
    "# Add a new column with the count of sentences\n",
    "df['sentence_count'] = df['sentences'].apply(count_sentences)\n",
    "\n",
    "# Calculate the percentage of NaN values in 'note_PS_manual' for each sentence count and cumulative NaN percentage\n",
    "result = {}\n",
    "cumulative_subset = pd.DataFrame()\n",
    "\n",
    "for count in sorted(df['sentence_count'].unique()):\n",
    "    subset = df[df['sentence_count'] == count]\n",
    "    nan_percentage = subset['note_PS_manual'].isna().mean() * 100\n",
    "    \n",
    "    cumulative_subset = pd.concat([cumulative_subset, subset])\n",
    "    cumulative_nan_percentage = cumulative_subset['note_PS_manual'].isna().mean() * 100\n",
    "    \n",
    "    result[count] = (nan_percentage, cumulative_nan_percentage)\n",
    "\n",
    "# Print the result\n",
    "for count, (nan_percentage, cumulative_nan_percentage) in result.items():\n",
    "    print(f\"{count}: {nan_percentage:.2f}% (Cumulative: {cumulative_nan_percentage:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac416d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "print(df['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53cb887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "df['sentences'] = df['sentences'].apply(ast.literal_eval)\n",
    "\n",
    "for sentence in df['sentences'][:10]:\n",
    "    print(type(sentence))\n",
    "    print(len(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0761a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
